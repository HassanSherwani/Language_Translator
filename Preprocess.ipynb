{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocess.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RX6yTTaKtav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t3V5W67K5zX",
        "colab_type": "text"
      },
      "source": [
        "# Machine Translation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q5A1ySzLPrq",
        "colab_type": "text"
      },
      "source": [
        "# 1)- Importing key modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPBH2WUkKwz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#support both Python 2 and Python 3 with minimal overhead.\n",
        "from __future__ import absolute_import, division, print_function\n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTJY_mj6K-LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string \n",
        "from string import digits\n",
        "from collections import Counter\n",
        "import re \n",
        "from sklearn.utils import shuffle\n",
        "import pandas as pd \n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "from unicodedata import normalize\n",
        "import matplotlib.pyplot as plt \n",
        "% matplotlib inline \n",
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-X0sC5fLS7a",
        "colab_type": "text"
      },
      "source": [
        "# 2)- Loading data\n",
        "\n",
        "We have data from 2009 to 2016."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFpuPSdKLEtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QpycswrLlaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split a loaded document into sentences\n",
        "def to_sentences(doc):\n",
        "\treturn doc.strip().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMLe1Lk5LnAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shortest and longest sentence lengths\n",
        "def sentence_lengths(sentences):\n",
        "\tlengths = [len(s.split()) for s in sentences]\n",
        "\treturn min(lengths), max(lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkrmOs7-L0X0",
        "colab_type": "text"
      },
      "source": [
        "### 2.1)- For year 2009"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg8f11ToLpnc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "311de7c7-f649-42f0-8cd9-ec431b5807f4"
      },
      "source": [
        "# load English data\n",
        "filename = 'newstest2009.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
        "\n",
        "# load French data\n",
        "filename = 'newstest2009.de'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('German data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English data: sentences=2525, min=1, max=108\n",
            "German data: sentences=2525, min=1, max=110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQmRuUHGL74N",
        "colab_type": "text"
      },
      "source": [
        "It is important to notice that sentence length is same. So, we have a balanced data.\n",
        "\n",
        "We shall check this on all years"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr3hYKrBMGtQ",
        "colab_type": "text"
      },
      "source": [
        "### 2.2)-For 2010"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF2HZBV-LycJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "acdf8664-6604-4301-8d9d-ba390eaf4347"
      },
      "source": [
        "# load English data\n",
        "filename = 'newstest2010.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
        "\n",
        "# load French data\n",
        "filename = 'newstest2010.de'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('German data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English data: sentences=2489, min=1, max=74\n",
            "German data: sentences=2489, min=1, max=86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFaKT4WENIA0",
        "colab_type": "text"
      },
      "source": [
        "### 2.3)-For year 2011"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFdlOFxaM8GF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "36ee1165-5d13-4dab-84dc-23b0f15cf953"
      },
      "source": [
        "# load English data\n",
        "filename = 'newstest2011.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
        "\n",
        "# load French data\n",
        "filename = 'newstest2011.de'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('German data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English data: sentences=3003, min=1, max=93\n",
            "German data: sentences=3003, min=1, max=92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-iKcf42NQl9",
        "colab_type": "text"
      },
      "source": [
        "### 2.4)-For year 2012"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6YRzm6yNO12",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9b944f1f-774e-458a-9039-0ccffa67ab29"
      },
      "source": [
        "# load English data\n",
        "filename = 'newstest2012.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
        "\n",
        "# load French data\n",
        "filename = 'newstest2012.de'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('German data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English data: sentences=3003, min=1, max=114\n",
            "German data: sentences=3003, min=1, max=101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4624P_5XNUCW",
        "colab_type": "text"
      },
      "source": [
        "### 2.5)-For year 2013"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pqUQn_dNW8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b7324f03-847e-44b7-9548-e0a01f752c87"
      },
      "source": [
        "# load English data\n",
        "filename = 'newstest2013.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
        "\n",
        "# load French data\n",
        "filename = 'newstest2013.de'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('German data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English data: sentences=3000, min=1, max=82\n",
            "German data: sentences=3000, min=1, max=85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ROs3kgYNXzl",
        "colab_type": "text"
      },
      "source": [
        "### 2.6)-For year 2014"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlS9fS1ANcTU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b6621ab2-a139-465e-96ce-f3f0fe6040bc"
      },
      "source": [
        "# load English data\n",
        "filename = 'newstest2014.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
        "\n",
        "# load French data\n",
        "filename = 'newstest2014.de'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('German data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English data: sentences=3003, min=1, max=68\n",
            "German data: sentences=3003, min=1, max=64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_By95YGUOKjP",
        "colab_type": "text"
      },
      "source": [
        "### 2.7)- For year 2015"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id289Xi1ONBv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d7d9604d-c494-4b87-b9c8-47cc10ba9d87"
      },
      "source": [
        "# load English data\n",
        "filename = 'newstest2015.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
        "\n",
        "# load French data\n",
        "filename = 'newstest2015.de'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('German data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English data: sentences=2169, min=1, max=71\n",
            "German data: sentences=2169, min=1, max=72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzgjV6OjON2G",
        "colab_type": "text"
      },
      "source": [
        "### For year 2016"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2fUwxLLOQFR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "53495f34-d4a9-4c63-a113-e531e115c426"
      },
      "source": [
        "# load English data\n",
        "filename = 'newstest2016.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
        "\n",
        "# load French data\n",
        "filename = 'newstest2016.de'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('German data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English data: sentences=2999, min=1, max=83\n",
            "German data: sentences=2999, min=1, max=88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jlgba1dOkje",
        "colab_type": "text"
      },
      "source": [
        "**All datasets have balanced sentences for English and German version**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "351hNk4lOsGe",
        "colab_type": "text"
      },
      "source": [
        "# 3)- Data Cleaning\n",
        "\n",
        "- Tokenizing text by white space.\n",
        "- Normalizing case to lowercase.\n",
        "- Removing punctuation from each word.\n",
        "- Removing non-printable characters.\n",
        "- Removing words that contain non-alphabetic characters.\n",
        "\n",
        "\n",
        "**We shall use only one file(2015) for quick processing.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4XeL-PkOh7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create cleaning function\n",
        "\n",
        "def clean_lines(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor line in lines:\n",
        "\t\t# normalize unicode characters\n",
        "\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\tline = line.decode('UTF-8')\n",
        "\t\t# tokenize on white space\n",
        "\t\tline = line.split()\n",
        "\t\t# convert to lower case\n",
        "\t\tline = [word.lower() for word in line]\n",
        "\t\t# remove punctuation from each token\n",
        "\t\tline = [word.translate(table) for word in line]\n",
        "\t\t# remove non-printable chars form each token\n",
        "\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t# remove tokens with numbers in them\n",
        "\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t# store as string\n",
        "\t\tcleaned.append(' '.join(line))\n",
        "\treturn cleaned"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXXFMVdbQIIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save a list of clean sentences to file\n",
        "def save_clean_sentences(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaYHZmrgQMJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e3c9597-9a43-4c9f-f9b9-a577c0f73775"
      },
      "source": [
        "# load English data\n",
        "filename = 'newstest2015.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "sentences = clean_lines(sentences)\n",
        "save_clean_sentences(sentences, 'english2015.pkl')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english2015.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb9xfJXJQeGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "9b1244e5-8d65-4cad-cc09-be937c555f71"
      },
      "source": [
        "# spot check for english version\n",
        "for i in range(10):\n",
        "\tprint(sentences[i])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "india and japan prime ministers meet in tokyo\n",
            "indias new prime minister narendra modi is meeting his japanese counterpart shinzo abe in tokyo to discuss economic and security ties on his first major foreign visit since winning mays election\n",
            "mr modi is on a fiveday trip to japan to strengthen economic ties with the third largest economy in the world\n",
            "high on the agenda are plans for greater nuclear cooperation\n",
            "india is also reportedly hoping for a deal on defence collaboration between the two nations\n",
            "karratha police arrest after high speed motorcycle chase\n",
            "a motorcycle has been seized after it was ridden at in a zone and through bushland to escape police in the pilbara\n",
            "traffic police on patrol in karratha this morning tried to pull over a blue motorcycle when they spotted it reaching as it pulled out of a service station on bathgate road\n",
            "police say the rider then failed to stop and continued on to burgess road before turning into bushland causing the officers to lose sight of it\n",
            "the motorcycle and a person matching the description of the rider was then spotted at a house on walcott way in bulgarra\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wryy3Y1OQVDv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "67adf2b8-d1a3-4b46-9715-d5d04f40f3ae"
      },
      "source": [
        "# load and check German data\n",
        "\n",
        "filename = 'newstest2015.de'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "sentences = clean_lines(sentences)\n",
        "save_clean_sentences(sentences, 'german2015.pkl')\n",
        "# spot check for german version\n",
        "for i in range(10):\n",
        "\tprint(sentences[i])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: german2015.pkl\n",
            "die premierminister indiens und japans trafen sich in tokio\n",
            "indiens neuer premierminister narendra modi trifft bei seinem ersten wichtigen auslandsbesuch seit seinem wahlsieg im mai seinen japanischen amtskollegen shinzo abe in toko um wirtschaftliche und sicherheitspolitische beziehungen zu besprechen\n",
            "herr modi befindet sich auf einer funftagigen reise nach japan um die wirtschaftlichen beziehungen mit der drittgroten wirtschaftsnation der welt zu festigen\n",
            "plane fur eine starkere kerntechnische zusammenarbeit stehen ganz oben auf der tagesordnung\n",
            "berichten zufolge hofft indien daruber hinaus auf einen vertrag zur verteidigungszusammenarbeit zwischen den beiden nationen\n",
            "polizei von karratha verhaftet nach schneller motorradjagd\n",
            "ein motorrad wurde beschlagnahmt nachdem der fahrer es mit kmh in einer kmhzone und durch buschland gefahren hatte um der polizei in bilbara zu entkommen\n",
            "verkehrspolizisten in karratha versuchten heute morgen ein blaues motorrad zu stoppen nachdem sie es dabei beobachtet hatten wie es mit kmh eine tankstelle auf der bathdate road verlie\n",
            "die polizei berichtet dass der fahrer die haltesignale dann ignorierte und weiter auf der burgess road fuhr bevor er in das buschland abbog wo die beamten es aus den augen verloren\n",
            "das motorrad sowie eine person die der beschreibung des fahrers entsprach wurden spater bei einem haus im walcott way in bulgarra gesehen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtx8CoZTRmG4",
        "colab_type": "text"
      },
      "source": [
        "# 4)- Checking Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLX2pg3KTqlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6iYQaotVRaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to save a list of clean sentences to file\n",
        "def save_clean_sentences(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_RVdnLCSCuo",
        "colab_type": "text"
      },
      "source": [
        "### 4.1)-frequency table\n",
        "create a frequency table for all words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icBWdxApRJyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_vocab(lines):\n",
        "\tvocab = Counter()\n",
        "\tfor line in lines:\n",
        "\t\ttokens = line.split()\n",
        "\t\tvocab.update(tokens)\n",
        "\treturn vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQIVl-zSSQdO",
        "colab_type": "text"
      },
      "source": [
        "### 4.2)- Trim Vocab\n",
        "\n",
        "Process the created vocabulary and remove all words from the Counter that have an occurrence below a specific threshold using *out of vocabulary (OOV)* method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBqCUzMPSbn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trim_vocab(vocab, min_occurance):\n",
        "\ttokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
        "\treturn set(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctXFtetOSjsf",
        "colab_type": "text"
      },
      "source": [
        "### 4.3)- Update dataset\n",
        "\n",
        "Update the sentences, remove all words not in the trimmed vocabulary and mark their removal with a special token, in this case, the string “unk“."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fymy1xb5Sf_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_dataset(lines, vocab):\n",
        "\tnew_lines = list()\n",
        "\tfor line in lines:\n",
        "\t\tnew_tokens = list()\n",
        "\t\tfor token in line.split():\n",
        "\t\t\tif token in vocab:\n",
        "\t\t\t\tnew_tokens.append(token)\n",
        "\t\t\telse:\n",
        "\t\t\t\tnew_tokens.append('unk')\n",
        "\t\tnew_line = ' '.join(new_tokens)\n",
        "\t\tnew_lines.append(new_line)\n",
        "\treturn new_lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfg7hEWQTApL",
        "colab_type": "text"
      },
      "source": [
        "### 4.4)- Check English version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs6lDK3RUZFR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "59e23cf3-b0af-4f99-d35a-1813c72c8f00"
      },
      "source": [
        "# load English dataset i.e pickled earlier\n",
        "filename = 'english2015.pkl'\n",
        "lines = load_clean_sentences(filename)\n",
        "\n",
        "# calculate vocabulary\n",
        "vocab = to_vocab(lines)\n",
        "print('English Vocabulary: %d' % len(vocab))\n",
        "\n",
        "# reduce vocabulary\n",
        "vocab = trim_vocab(vocab, 5)\n",
        "print('New English Vocabulary: %d' % len(vocab))\n",
        "\n",
        "# mark out of vocabulary words\n",
        "lines = update_dataset(lines, vocab)\n",
        "# save updated dataset\n",
        "filename = 'english_vocab2015.pkl'\n",
        "save_clean_sentences(lines, filename)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary: 7227\n",
            "New English Vocabulary: 1210\n",
            "Saved: english_vocab2015.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A4kJkxAW79J",
        "colab_type": "text"
      },
      "source": [
        "### 4.5)-Check German version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv3bZhbTUyPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "235a6d72-0425-4fe9-98f6-9df8e31f2312"
      },
      "source": [
        "# load English dataset i.e pickled earlier\n",
        "filename = 'german2015.pkl'\n",
        "lines = load_clean_sentences(filename)\n",
        "\n",
        "# calculate vocabulary\n",
        "vocab = to_vocab(lines)\n",
        "print('German Vocabulary: %d' % len(vocab))\n",
        "\n",
        "# reduce vocabulary\n",
        "vocab = trim_vocab(vocab, 5)\n",
        "print('New German Vocabulary: %d' % len(vocab))\n",
        "\n",
        "# mark out of vocabulary words\n",
        "lines = update_dataset(lines, vocab)\n",
        "# save updated dataset\n",
        "filename = 'german_vocab2015.pkl'\n",
        "save_clean_sentences(lines, filename)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "German Vocabulary: 9280\n",
            "New German Vocabulary: 988\n",
            "Saved: german_vocab2015.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1ZW_PsfXOc8",
        "colab_type": "text"
      },
      "source": [
        "These cleaning and vocab reduction methods have given us a more normalized and potent data for model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4hOyoLwXZsL",
        "colab_type": "text"
      },
      "source": [
        "# 5)- Apply to all data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7ADOSOiXMJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}