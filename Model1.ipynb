{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Model1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QJxJLfT6m4zo"
      },
      "source": [
        "# Machine Translation\n",
        "\n",
        "English-German Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YvFSu3KOnCQI"
      },
      "source": [
        "# 1)- Importing key modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ydSdCo4JzRta",
        "colab": {}
      },
      "source": [
        "#support both Python 2 and Python 3 with minimal overhead.\n",
        "from __future__ import absolute_import, division, print_function\n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HXdjBXc-zNXe",
        "colab": {}
      },
      "source": [
        "import string \n",
        "import re \n",
        "from numpy import array, argmax, random, take \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline \n",
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ElRcz5K3hJrC",
        "outputId": "99692ddd-18cb-43e5-b489-f0539437b0f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model \n",
        "from keras import optimizers \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XN_b91atnHye"
      },
      "source": [
        "# 2)- Reading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmRrgEJZotlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to read raw text file\n",
        "def read_text(filename): \n",
        "        # open the file \n",
        "        file = open(filename, mode='rt', encoding='utf-8') \n",
        "        \n",
        "        # read all text \n",
        "        text = file.read() \n",
        "        file.close() \n",
        "        return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBghzTLCo3zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split text into sentences \n",
        "def to_lines(text): \n",
        "      sents = text.strip().split('\\n') \n",
        "      sents = [i.split('\\t') for i in sents] \n",
        "      return sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMRC7IBiuxAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = read_text(\"merged2015.txt\") \n",
        "deu_eng = to_lines(data) \n",
        "deu_eng = array(deu_eng)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5azilzTootrW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5722b2ec-3506-4302-e2e1-479f2ad81c09"
      },
      "source": [
        "type(deu_eng)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSFX69O1ns-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "faf95118-b030-4d11-b622-ed23c95b6aae"
      },
      "source": [
        "deu_eng[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['india and japan prime ministers meet in tokyo',\n",
              "       'die premierminister indiens und japans trafen sich in tokio'],\n",
              "      dtype='<U511')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o1FK1T2Kicbi",
        "outputId": "65a0cb18-6b78-427a-e07e-1b9d978bef62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "# for english part \n",
        "deu_eng[:,0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['india and japan prime ministers meet in tokyo',\n",
              "       'indias new prime minister narendra modi is meeting his japanese counterpart shinzo abe in tokyo to discuss economic and security ties on his first major foreign visit since winning mays election',\n",
              "       'mr modi is on a fiveday trip to japan to strengthen economic ties with the third largest economy in the world',\n",
              "       ..., 'five minutes later the first mountainbikers set off',\n",
              "       'bent hansen chairman of the association cycling on the grosser feldberg gave the starting orders and wished those taking part an enjoyable trip',\n",
              "       'next year he hopes to have safety barriers on the course for the benefit of those taking part on the feldberg'],\n",
              "      dtype='<U511')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z2CAJPD_ifVc",
        "outputId": "1ba88580-04de-4e01-940e-44dd33e72782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "# for german part of lang.\n",
        "deu_eng[:,1]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['die premierminister indiens und japans trafen sich in tokio',\n",
              "       'indiens neuer premierminister narendra modi trifft bei seinem ersten wichtigen auslandsbesuch seit seinem wahlsieg im mai seinen japanischen amtskollegen shinzo abe in toko um wirtschaftliche und sicherheitspolitische beziehungen zu besprechen',\n",
              "       'herr modi befindet sich auf einer funftagigen reise nach japan um die wirtschaftlichen beziehungen mit der drittgroten wirtschaftsnation der welt zu festigen',\n",
              "       ..., 'funf minuten spater legten die ersten mountainbiker los',\n",
              "       'bent hansen vorsitzender des vereins radeln auf den groen feldberg gab die startkommandos und wunschte den teilnehmern einen schonen ausflug',\n",
              "       'fur nachstes jahr hoffe er dass es gelingt die strecke zum feldberg hinauf zur sicherheit der teilnehmer zu sperren'],\n",
              "      dtype='<U511')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUZ6BSmvtJ1s",
        "colab_type": "text"
      },
      "source": [
        "# 3)-Text Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GEia-m7ViPLD"
      },
      "source": [
        "### 3.1)-Text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmC81k-wh8ZJ",
        "colab": {}
      },
      "source": [
        "# Remove punctuation \n",
        "deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]] \n",
        "deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]] \n",
        "\n",
        "# convert text to lowercase \n",
        "for i in range(len(deu_eng)): \n",
        "    deu_eng[i,0] = deu_eng[i,0].lower() \n",
        "    deu_eng[i,1] = deu_eng[i,1].lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4FdyDdXwiS9i"
      },
      "source": [
        "### 3.2)-Text to Sequence Conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8pxu1-xNh8cA",
        "outputId": "d83d9a5d-f844-443b-d316-070326891731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# empty lists \n",
        "eng_l = [] \n",
        "deu_l = [] \n",
        "\n",
        "# populate the lists with sentence lengths \n",
        "for i in deu_eng[:,0]: \n",
        "      eng_l.append(len(i.split())) \n",
        "\n",
        "for i in deu_eng[:,1]: \n",
        "      deu_l.append(len(i.split())) \n",
        "\n",
        "length_df = pd.DataFrame({'eng':eng_l, 'deu':deu_l})\n",
        "length_df.hist(bins = 30) \n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2De4ZERJzX_4"
      },
      "source": [
        "the maximum length of the German sentences is 78 and that of the English phrases is 75."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lpt6tCjfkGKu"
      },
      "source": [
        "### 3.3)-vectorize our text data \n",
        "\n",
        "by using Kerasâ€™s Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_j_y_uZ7h8e6",
        "outputId": "905cf2b6-8a00-4ecb-8e74-79b58b5c0f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# function to build a tokenizer \n",
        "def tokenization(lines): \n",
        "      tokenizer = Tokenizer() \n",
        "      tokenizer.fit_on_texts(lines) \n",
        "      return tokenizer\n",
        "\n",
        "# prepare english tokenizer \n",
        "eng_tokenizer = tokenization(deu_eng[:, 0]) \n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1 \n",
        "eng_length = 8 \n",
        "\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 7231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0ha_ZNySh8hh",
        "outputId": "55e86e06-59bd-43c1-bff0-b208ec75e6b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# prepare Deutch tokenizer \n",
        "deu_tokenizer = tokenization(deu_eng[:, 1]) \n",
        "deu_vocab_size = len(deu_tokenizer.word_index) + 1 \n",
        "deu_length = 8 \n",
        "print('Deutch Vocabulary Size: %d' % deu_vocab_size)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deutch Vocabulary Size: 9284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XvmowmplzfdT"
      },
      "source": [
        "There is difference in amount of words in two languages.We need to encode sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cUY-Py6WzhtB"
      },
      "source": [
        "### 3.4)-encode and pad sequences "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QElAizsykOtJ",
        "colab": {}
      },
      "source": [
        "def encode_sequences(tokenizer, length, lines):          \n",
        "         # integer encode sequences          \n",
        "         seq = tokenizer.texts_to_sequences(lines)          \n",
        "         # pad sequences with 0 values          \n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')           \n",
        "         return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "408A3w7Ukb4r"
      },
      "source": [
        "# 4)-Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kJF5iWUOnUgC"
      },
      "source": [
        "### 4.1)- Train-test Split\n",
        "\n",
        "80%-20%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GzxozTM0kZk7",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "# split data into train and test set \n",
        "train,test= train_test_split(deu_eng,test_size=0.2,random_state= 12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xvyO7SdQkr88"
      },
      "source": [
        "### 4.2)- Defining input and target\n",
        "We will encode English sentences as the input sequences and German sentences as the target sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wJvJMU9Vzr-7",
        "outputId": "93e6db1e-1485-4245-e2b9-b8b054d158b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# english version\n",
        "train[:, 0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['its not my responsibility',\n",
              "       'meanwhile in our homes items as innocuous as floor tiles or shed roofs have routinely contained asbestos',\n",
              "       'she said i keep thinking this world did not get better within these years',\n",
              "       ...,\n",
              "       'crops are rotting in the fields mines have been deserted and the markets have been abandoned the virus has cost the region dearly',\n",
              "       'the preparations for the party are well underway in tannenwald gun club which will celebrate years since being established on to september',\n",
              "       'it also means higher taxes'], dtype='<U511')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TcMT6wFoztLT",
        "outputId": "b6e7c626-bb8e-4351-967b-887175b67f1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# english version\n",
        "train[:, 1]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ich bin nicht dafur verantwortlich',\n",
              "       'und derweil haben so unschuldige gegenstande in unseren hausern wie fubodenplatten oder schuppendacher standardmaig asbest enthalten',\n",
              "       'sie sagte ich denke immer dass diese welt in diesen jahren nicht besser geworden ist',\n",
              "       ...,\n",
              "       'die ernte verrottet auf den feldern die minen sind verlassen und die markte verwaist das virus hat der region schwer zugesetzt',\n",
              "       'auf hochtouren laufen beim schutzenverein tannenwald die vorbereitungen fur das grundungsfest von bis september',\n",
              "       'sie bedeutet auch hohere steuern'], dtype='<U511')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P6ZhWiJdkkb8",
        "colab": {}
      },
      "source": [
        "# prepare training data \n",
        "trainX = encode_sequences(eng_tokenizer, deu_length, train[:, 0]) \n",
        "trainY = encode_sequences(deu_tokenizer, eng_length, train[:, 1]) \n",
        "\n",
        "# prepare validation data \n",
        "testX = encode_sequences(eng_tokenizer, deu_length, test[:, 0]) \n",
        "testY = encode_sequences(deu_tokenizer, eng_length, test[:, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f2LKJrFfz0GI",
        "outputId": "8bae2b05-7bbb-4d8b-a3f1-0dec551f2499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print(trainX[:5])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  48   23   51 4075    0    0    0    0]\n",
            " [2904   64 2016 5284   17 5285 2943   65]\n",
            " [ 141   97   23   98  179  362  193   58]\n",
            " [  69  610  427   23 1523 1892 2202 5445]\n",
            " [1329    3  113   76    1  204 3174    4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L81CkA-Rz0Jh",
        "outputId": "6a42b813-d99d-42d5-e255-73747cf4c055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print(trainY[:5])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  15  184   18  272 1521    0    0    0]\n",
            " [1071   30 2751   59 6215 6216   85 1223]\n",
            " [ 197    4  261  100   18  323  708   16]\n",
            " [6478   55 1233  208 6479 6480  956 1466]\n",
            " [ 264    5  550    6    4  592    6  180]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "plUoF_-gz0Mh",
        "outputId": "951ab99a-2c3b-4dab-b5af-6a47456852df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(trainX.shape)\n",
        "print(trainY.shape)\n",
        "print(testX.shape)\n",
        "print(testY.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5205, 8)\n",
            "(5205, 8)\n",
            "(1302, 8)\n",
            "(1302, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7ubte9AUngfx"
      },
      "source": [
        "### 4.3)- build NMT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CuEMUhwV248X",
        "outputId": "7b45f9a4-0337-4a40-d333-0897178fd2fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "eng_length"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "afiv_7mw26av",
        "outputId": "8a9dc41a-71f0-4091-c3c9-109500ca1b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "deu_length"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yORpCfMCkkjz",
        "colab": {}
      },
      "source": [
        " def build_model(in_vocab,out_vocab, in_timesteps,out_timesteps,n):   \n",
        "      model = Sequential() \n",
        "      model.add(Embedding(in_vocab, n, input_length=in_timesteps,   \n",
        "      mask_zero=True)) \n",
        "      model.add(LSTM(n)) \n",
        "      model.add(RepeatVector(out_timesteps)) \n",
        "      model.add(LSTM(n, return_sequences=True))  \n",
        "      model.add(Dense(out_vocab, activation='softmax')) \n",
        "      return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H_0LMdC6lgnp",
        "outputId": "27b19d28-808c-48a7-dc80-32dfde5b5762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# model compilation (with 512 hidden units)\n",
        "model = build_model(eng_vocab_size,deu_vocab_size, eng_length, deu_length, 512)\n",
        "\n",
        "rms = optimizers.RMSprop(lr=0.001) \n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BA8KW2_80Hs3",
        "outputId": "bc896441-8752-433a-ba93-4d9e37ac117d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 8, 512)            3702272   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 8, 512)            0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 8, 512)            2099200   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8, 9284)           4762692   \n",
            "=================================================================\n",
            "Total params: 12,663,364\n",
            "Trainable params: 12,663,364\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9M1ukVFAlkFz",
        "outputId": "01ca66b8-aef7-4416-cfec-a55f8ff0c9bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "filename = 'model_translate.h1' \n",
        "\n",
        "# set checkpoint\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss',  \n",
        "                             verbose=1, save_best_only=True, \n",
        "                             mode='min') \n",
        "\n",
        "\n",
        "# train model \n",
        "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), \n",
        "                    epochs=30, batch_size=512, validation_split = 0.2, \n",
        "                    callbacks=[checkpoint], verbose=1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Train on 4164 samples, validate on 1041 samples\n",
            "Epoch 1/30\n",
            "4164/4164 [==============================] - 5s 1ms/step - loss: 8.2332 - val_loss: 7.3532\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.35320, saving model to model_translate.h1\n",
            "Epoch 2/30\n",
            "4164/4164 [==============================] - 2s 560us/step - loss: 7.0969 - val_loss: 7.2199\n",
            "\n",
            "Epoch 00002: val_loss improved from 7.35320 to 7.21986, saving model to model_translate.h1\n",
            "Epoch 3/30\n",
            "4164/4164 [==============================] - 2s 560us/step - loss: 6.9787 - val_loss: 7.1557\n",
            "\n",
            "Epoch 00003: val_loss improved from 7.21986 to 7.15569, saving model to model_translate.h1\n",
            "Epoch 4/30\n",
            "4164/4164 [==============================] - 2s 556us/step - loss: 6.9276 - val_loss: 7.1514\n",
            "\n",
            "Epoch 00004: val_loss improved from 7.15569 to 7.15144, saving model to model_translate.h1\n",
            "Epoch 5/30\n",
            "4164/4164 [==============================] - 2s 556us/step - loss: 6.8596 - val_loss: 7.1162\n",
            "\n",
            "Epoch 00005: val_loss improved from 7.15144 to 7.11618, saving model to model_translate.h1\n",
            "Epoch 6/30\n",
            "4164/4164 [==============================] - 2s 562us/step - loss: 6.8101 - val_loss: 7.0244\n",
            "\n",
            "Epoch 00006: val_loss improved from 7.11618 to 7.02436, saving model to model_translate.h1\n",
            "Epoch 7/30\n",
            "4164/4164 [==============================] - 2s 557us/step - loss: 6.7856 - val_loss: 7.0115\n",
            "\n",
            "Epoch 00007: val_loss improved from 7.02436 to 7.01148, saving model to model_translate.h1\n",
            "Epoch 8/30\n",
            "4164/4164 [==============================] - 2s 558us/step - loss: 6.7015 - val_loss: 6.9954\n",
            "\n",
            "Epoch 00008: val_loss improved from 7.01148 to 6.99535, saving model to model_translate.h1\n",
            "Epoch 9/30\n",
            "4164/4164 [==============================] - 2s 558us/step - loss: 6.6527 - val_loss: 7.0541\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 6.99535\n",
            "Epoch 10/30\n",
            "4164/4164 [==============================] - 2s 560us/step - loss: 6.6337 - val_loss: 6.8819\n",
            "\n",
            "Epoch 00010: val_loss improved from 6.99535 to 6.88194, saving model to model_translate.h1\n",
            "Epoch 11/30\n",
            "4164/4164 [==============================] - 2s 566us/step - loss: 6.5684 - val_loss: 6.9786\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 6.88194\n",
            "Epoch 12/30\n",
            "4164/4164 [==============================] - 2s 564us/step - loss: 6.5219 - val_loss: 6.8840\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 6.88194\n",
            "Epoch 13/30\n",
            "4164/4164 [==============================] - 2s 561us/step - loss: 6.4682 - val_loss: 6.8279\n",
            "\n",
            "Epoch 00013: val_loss improved from 6.88194 to 6.82789, saving model to model_translate.h1\n",
            "Epoch 14/30\n",
            "4164/4164 [==============================] - 2s 563us/step - loss: 6.3995 - val_loss: 6.7399\n",
            "\n",
            "Epoch 00014: val_loss improved from 6.82789 to 6.73991, saving model to model_translate.h1\n",
            "Epoch 15/30\n",
            "4164/4164 [==============================] - 2s 568us/step - loss: 6.3723 - val_loss: 6.8456\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 6.73991\n",
            "Epoch 16/30\n",
            "4164/4164 [==============================] - 2s 561us/step - loss: 6.3227 - val_loss: 6.6523\n",
            "\n",
            "Epoch 00016: val_loss improved from 6.73991 to 6.65228, saving model to model_translate.h1\n",
            "Epoch 17/30\n",
            "4164/4164 [==============================] - 2s 568us/step - loss: 6.2340 - val_loss: 6.7842\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 6.65228\n",
            "Epoch 18/30\n",
            "4164/4164 [==============================] - 2s 562us/step - loss: 6.2349 - val_loss: 6.5691\n",
            "\n",
            "Epoch 00018: val_loss improved from 6.65228 to 6.56914, saving model to model_translate.h1\n",
            "Epoch 19/30\n",
            "4164/4164 [==============================] - 2s 570us/step - loss: 6.1145 - val_loss: 6.6953\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 6.56914\n",
            "Epoch 20/30\n",
            "4164/4164 [==============================] - 2s 564us/step - loss: 6.1321 - val_loss: 6.5846\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 6.56914\n",
            "Epoch 21/30\n",
            "4164/4164 [==============================] - 2s 564us/step - loss: 6.0686 - val_loss: 6.5383\n",
            "\n",
            "Epoch 00021: val_loss improved from 6.56914 to 6.53825, saving model to model_translate.h1\n",
            "Epoch 22/30\n",
            "4164/4164 [==============================] - 2s 570us/step - loss: 6.0085 - val_loss: 6.5072\n",
            "\n",
            "Epoch 00022: val_loss improved from 6.53825 to 6.50718, saving model to model_translate.h1\n",
            "Epoch 23/30\n",
            "4164/4164 [==============================] - 3s 606us/step - loss: 5.9707 - val_loss: 6.4585\n",
            "\n",
            "Epoch 00023: val_loss improved from 6.50718 to 6.45845, saving model to model_translate.h1\n",
            "Epoch 24/30\n",
            "4164/4164 [==============================] - 2s 565us/step - loss: 5.9011 - val_loss: 6.4301\n",
            "\n",
            "Epoch 00024: val_loss improved from 6.45845 to 6.43013, saving model to model_translate.h1\n",
            "Epoch 25/30\n",
            "4164/4164 [==============================] - 2s 563us/step - loss: 5.8753 - val_loss: 6.3337\n",
            "\n",
            "Epoch 00025: val_loss improved from 6.43013 to 6.33370, saving model to model_translate.h1\n",
            "Epoch 26/30\n",
            "4164/4164 [==============================] - 2s 569us/step - loss: 5.8286 - val_loss: 6.2938\n",
            "\n",
            "Epoch 00026: val_loss improved from 6.33370 to 6.29383, saving model to model_translate.h1\n",
            "Epoch 27/30\n",
            "4164/4164 [==============================] - 2s 563us/step - loss: 5.7594 - val_loss: 6.2806\n",
            "\n",
            "Epoch 00027: val_loss improved from 6.29383 to 6.28063, saving model to model_translate.h1\n",
            "Epoch 28/30\n",
            "4164/4164 [==============================] - 2s 567us/step - loss: 5.7206 - val_loss: 6.2800\n",
            "\n",
            "Epoch 00028: val_loss improved from 6.28063 to 6.28004, saving model to model_translate.h1\n",
            "Epoch 29/30\n",
            "4164/4164 [==============================] - 2s 560us/step - loss: 5.6972 - val_loss: 6.1837\n",
            "\n",
            "Epoch 00029: val_loss improved from 6.28004 to 6.18366, saving model to model_translate.h1\n",
            "Epoch 30/30\n",
            "4164/4164 [==============================] - 2s 568us/step - loss: 5.6134 - val_loss: 6.2250\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 6.18366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nwrL0_sjmUuO"
      },
      "source": [
        "# 5)-Plotting results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iUYdtjQXlupF",
        "outputId": "c2c948cf-25cf-42a9-fac9-82cacbaf719a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "plt.plot(history.history['loss']) \n",
        "plt.plot(history.history['val_loss']) \n",
        "plt.legend(['train','validation']) \n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xQVCiaNDn4VJ"
      },
      "source": [
        "As you can see in the above plot, the validation loss stopped decreasing after 25 epochs. It did improve but not very significant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FgePSmZBmSbn"
      },
      "source": [
        "# 6)-Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6gh_CukCl7Al",
        "colab": {}
      },
      "source": [
        "model = load_model('model_translate.h1') \n",
        "\n",
        "preds = model.predict_classes(testX.reshape((testX.shape[0], testX.shape[1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M-DaR0x4oA4P"
      },
      "source": [
        "These predictions are sequences of integers. We need to convert these integers to their corresponding words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VU6IOmOOoB3f"
      },
      "source": [
        "### 6.1)- Convert integers to words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oiy9gGAAmAat",
        "colab": {}
      },
      "source": [
        "def get_word(n, tokenizer):  \n",
        "      for word, index in tokenizer.word_index.items():                       \n",
        "          if index == n: \n",
        "              return word \n",
        "      return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KqSwZTcXmDTW"
      },
      "source": [
        "### 6.2)-Convert predictions into text (German)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y82jlYQMmAg0",
        "colab": {}
      },
      "source": [
        "preds_text = [] \n",
        "for i in preds:        \n",
        "       temp = []        \n",
        "       for j in range(len(i)):             \n",
        "            t = get_word(i[j], deu_tokenizer)             \n",
        "            if j > 0:                 \n",
        "                if (t==get_word(i[j-1],deu_tokenizer))or(t== None):                       \n",
        "                     temp.append('')                 \n",
        "                else:                      \n",
        "                     temp.append(t)             \n",
        "            else:                    \n",
        "                if(t == None):                                   \n",
        "                     temp.append('')                    \n",
        "                else:                           \n",
        "                     temp.append(t)        \n",
        "       preds_text.append(' '.join(temp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XOAI5cB8mAj3",
        "colab": {}
      },
      "source": [
        "pred_df = pd.DataFrame({'actual' : test[:,1], 'predicted' : preds_text})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S2orsrv6oo4o",
        "outputId": "55f373a5-0c76-4c7e-8a3b-cfead7ad2f97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "# 1st 5 rows\n",
        "pred_df.head(5)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wahrend die konkurrenz naher ruckt und neue technologien alte bequeme sicherheiten bedrohen ist die anstehende verhandlung der lizenzgebuhren fur die bbc mit besonderen gefahren verbunden</td>\n",
              "      <td>und die  und  der zu wurde</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>luther rabinowitz lie seine pyramide zusammenfallen</td>\n",
              "      <td>kontaktierte ehemann war in fur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kurzlich erklarten die gesundheitsbehorden die krankheit sei in jeden winkel des landes vorgedrungen</td>\n",
              "      <td>der die  in   zu werden</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ich billige den tweet nicht und habe ihn geloscht hie es in der spateren nachricht</td>\n",
              "      <td>und     der zu war</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fur den chef des oberurseler kultkiosks gehoren radtouren uber den feldberg einfach dazu</td>\n",
              "      <td>der  die in   zu</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                        actual                           predicted\n",
              "0  wahrend die konkurrenz naher ruckt und neue technologien alte bequeme sicherheiten bedrohen ist die anstehende verhandlung der lizenzgebuhren fur die bbc mit besonderen gefahren verbunden          und die  und  der zu wurde\n",
              "1                                                                                                                                          luther rabinowitz lie seine pyramide zusammenfallen  kontaktierte ehemann war in fur   \n",
              "2                                                                                         kurzlich erklarten die gesundheitsbehorden die krankheit sei in jeden winkel des landes vorgedrungen             der die  in   zu werden\n",
              "3                                                                                                           ich billige den tweet nicht und habe ihn geloscht hie es in der spateren nachricht                  und     der zu war\n",
              "4                                                                                                     fur den chef des oberurseler kultkiosks gehoren radtouren uber den feldberg einfach dazu                   der  die in   zu "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LPocsO00mJy-",
        "outputId": "ba0327ef-fcad-4fbf-c233-464e3cbe47b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "# print 5 rows randomly \n",
        "pred_df.sample(5)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1264</th>\n",
              "      <td>auch die bezirksligastaffel der scwjungen dort startet noch torpedo ii ist nicht komplett</td>\n",
              "      <td>ich dass   und nicht  war</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>901</th>\n",
              "      <td>wir denken nicht dass es lange her ist ich meine es ware gestern geschehen einige szenen aus diesen tagen verfolgen mich immerzu</td>\n",
              "      <td>ich war  und nicht zu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>553</th>\n",
              "      <td>ptvdirektor athar farooq sagte die mit stocken ausgerusteten demonstranten hatten gerate in der redaktion beschadigt</td>\n",
              "      <td>und     der zu war</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1131</th>\n",
              "      <td>sein hochklassiges team von rechtsbeistanden beantragte bei dem mit dem fall betrauten richter die anklage abzuweisen und argumentierte dass das gesetz mit dem gegen den dienstaltesten gouverneur ...</td>\n",
              "      <td>und  dass und  nicht  war</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>seiner ansicht nach konnten alle mitglieder beider vereine kunftig wieder an einem strang ziehen</td>\n",
              "      <td>der  die in   zu</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                       actual                  predicted\n",
              "1264                                                                                                                auch die bezirksligastaffel der scwjungen dort startet noch torpedo ii ist nicht komplett  ich dass   und nicht  war\n",
              "901                                                                          wir denken nicht dass es lange her ist ich meine es ware gestern geschehen einige szenen aus diesen tagen verfolgen mich immerzu    ich war  und nicht zu  \n",
              "553                                                                                      ptvdirektor athar farooq sagte die mit stocken ausgerusteten demonstranten hatten gerate in der redaktion beschadigt         und     der zu war\n",
              "1131  sein hochklassiges team von rechtsbeistanden beantragte bei dem mit dem fall betrauten richter die anklage abzuweisen und argumentierte dass das gesetz mit dem gegen den dienstaltesten gouverneur ...  und  dass und  nicht  war\n",
              "28                                                                                                           seiner ansicht nach konnten alle mitglieder beider vereine kunftig wieder an einem strang ziehen          der  die in   zu "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}