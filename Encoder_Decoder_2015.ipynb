{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Encoder_Decoder_2015.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QJxJLfT6m4zo"
      },
      "source": [
        "# Machine Translation\n",
        "\n",
        "English-German Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YvFSu3KOnCQI"
      },
      "source": [
        "# 1)- Importing key modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ydSdCo4JzRta",
        "colab": {}
      },
      "source": [
        "#support both Python 2 and Python 3 with minimal overhead.\n",
        "from __future__ import absolute_import, division, print_function\n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eXS6Ad_pwdsT",
        "colab": {}
      },
      "source": [
        "# What's life without style :). So, let's add style to our dataframes\n",
        "#from IPython.core.display import HTML\n",
        "#css = open('style-table.css').read() + open('style-notebook.css').read()\n",
        "#HTML('<style>{}</style>'.format(css))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HXdjBXc-zNXe",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "import string \n",
        "import pickle\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "from string import digits\n",
        "import re \n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import array, argmax, random, take \n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline \n",
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ElRcz5K3hJrC",
        "outputId": "84eb1007-95b4-460b-ad08-62888928ef22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, LSTM, Embedding,Input,RepeatVector\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model \n",
        "from keras import optimizers "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yI5oZH-mwmWL",
        "outputId": "1025df34-01fa-43e8-8412-e21e2ef48ad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "!  pip install version_information"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting version_information\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b0/6088e15b9ac43a08ccd300d68e0b900a20cf62077596c11ad11dd8cc9e4b/version_information-1.0.3.tar.gz\n",
            "Building wheels for collected packages: version-information\n",
            "  Building wheel for version-information (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for version-information: filename=version_information-1.0.3-cp36-none-any.whl size=3880 sha256=f80d6198f11f19324bb24ce3d49bc7b5011a81224c71c1c63783cdda74c23891\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/4c/b3/1976ac11dbd802723b564de1acaa453a72c36c95827e576321\n",
            "Successfully built version-information\n",
            "Installing collected packages: version-information\n",
            "Successfully installed version-information-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "91NCx_C7wdsZ",
        "outputId": "03880baa-4663-4707-faeb-2c6549af9bda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# first install: pip install version_information\n",
        "%reload_ext version_information\n",
        "%version_information pandas,re,sklearn, matplotlib,keras"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "\\begin{tabular}{|l|l|}\\hline\n{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\nPython & 3.6.8 64bit [GCC 8.0.1 20180414 (experimental) [trunk revision 259383] \\\\ \\hline\nIPython & 5.5.0 \\\\ \\hline\nOS & Linux 4.14.137+ x86\\_64 with Ubuntu 18.04 bionic \\\\ \\hline\npandas & 0.24.2 \\\\ \\hline\nre & 2.2.1 \\\\ \\hline\nsklearn & 0.21.3 \\\\ \\hline\nmatplotlib & 3.0.3 \\\\ \\hline\nkeras & 2.2.5 \\\\ \\hline\n\\hline \\multicolumn{2}{|l|}{Tue Sep 10 18:09:04 2019 UTC} \\\\ \\hline\n\\end{tabular}\n",
            "application/json": {
              "Software versions": [
                {
                  "version": "3.6.8 64bit [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]",
                  "module": "Python"
                },
                {
                  "version": "5.5.0",
                  "module": "IPython"
                },
                {
                  "version": "Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic",
                  "module": "OS"
                },
                {
                  "version": "0.24.2",
                  "module": "pandas"
                },
                {
                  "version": "2.2.1",
                  "module": "re"
                },
                {
                  "version": "0.21.3",
                  "module": "sklearn"
                },
                {
                  "version": "3.0.3",
                  "module": "matplotlib"
                },
                {
                  "version": "2.2.5",
                  "module": "keras"
                }
              ]
            },
            "text/html": [
              "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.8 64bit [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]</td></tr><tr><td>IPython</td><td>5.5.0</td></tr><tr><td>OS</td><td>Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic</td></tr><tr><td>pandas</td><td>0.24.2</td></tr><tr><td>re</td><td>2.2.1</td></tr><tr><td>sklearn</td><td>0.21.3</td></tr><tr><td>matplotlib</td><td>3.0.3</td></tr><tr><td>keras</td><td>2.2.5</td></tr><tr><td colspan='2'>Tue Sep 10 18:09:04 2019 UTC</td></tr></table>"
            ],
            "text/plain": [
              "Software versions\n",
              "Python 3.6.8 64bit [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]\n",
              "IPython 5.5.0\n",
              "OS Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic\n",
              "pandas 0.24.2\n",
              "re 2.2.1\n",
              "sklearn 0.21.3\n",
              "matplotlib 3.0.3\n",
              "keras 2.2.5\n",
              "Tue Sep 10 18:09:04 2019 UTC"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XN_b91atnHye"
      },
      "source": [
        "# 2)- Reading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZM1ZO53WhL_6",
        "colab": {}
      },
      "source": [
        "lines= pd.read_pickle('data_2015.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gws8XMBx-IfV",
        "outputId": "acda1adb-e26a-429e-fb00-4d3990f3d8c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lines.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2169, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GdbfP26dHHIX",
        "outputId": "fd2a74b8-6e32-46e7-8990-e84564246646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "lines.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>ger</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>unk and unk prime unk unk in unk</td>\n",
              "      <td>die premierminister unk und unk unk sich in unk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>unk new prime minister unk unk is meeting his unk unk unk unk in unk to unk economic and security unk on his first major foreign unk since unk unk election</td>\n",
              "      <td>unk unk premierminister unk unk unk bei seinem ersten unk unk seit seinem unk im mai seinen unk unk unk unk in unk um unk und unk unk zu unk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mr unk is on a unk unk to unk to unk economic unk with the third largest economy in the world</td>\n",
              "      <td>herr unk unk sich auf einer unk unk nach unk um die wirtschaftlichen unk mit der unk unk der welt zu unk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>high on the unk are plans for unk unk unk</td>\n",
              "      <td>plane fur eine unk unk unk stehen ganz oben auf der unk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>unk is also unk unk for a deal on unk unk between the two unk</td>\n",
              "      <td>berichten zufolge hofft unk daruber hinaus auf einen unk zur unk zwischen den beiden unk</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                           eng                                                                                                                                           ger\n",
              "0                                                                                                                             unk and unk prime unk unk in unk                                                                                               die premierminister unk und unk unk sich in unk\n",
              "1  unk new prime minister unk unk is meeting his unk unk unk unk in unk to unk economic and security unk on his first major foreign unk since unk unk election  unk unk premierminister unk unk unk bei seinem ersten unk unk seit seinem unk im mai seinen unk unk unk unk in unk um unk und unk unk zu unk\n",
              "2                                                                mr unk is on a unk unk to unk to unk economic unk with the third largest economy in the world                                      herr unk unk sich auf einer unk unk nach unk um die wirtschaftlichen unk mit der unk unk der welt zu unk\n",
              "3                                                                                                                    high on the unk are plans for unk unk unk                                                                                       plane fur eine unk unk unk stehen ganz oben auf der unk\n",
              "4                                                                                                unk is also unk unk for a deal on unk unk between the two unk                                                      berichten zufolge hofft unk daruber hinaus auf einen unk zur unk zwischen den beiden unk"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8532Vnqswdsh"
      },
      "source": [
        "As this is big data and I have a poor old computing machine. So, I ll use smaller sample. It got to be random to avoid sample biaseness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PZuWRYCZwdsh",
        "outputId": "31ed1025-08d1-45de-8af0-9dca4c33824a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "lines.sample(15)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>ger</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1502</th>\n",
              "      <td>unk unk wanted</td>\n",
              "      <td>unk unk unk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1744</th>\n",
              "      <td>and that goes for all generations</td>\n",
              "      <td>und das unk durch die generationen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>669</th>\n",
              "      <td>im unk that way</td>\n",
              "      <td>ich bin da ein bisschen unk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>unk unk tv series and parts in films unk his next unk but after his bbc series unk he thought what are you going to do next unk because it all unk a bit like youre unk time or youre unk going unk</td>\n",
              "      <td>unk unk unk und unk unk sein nachstes unk aber nach seiner unk unk dachte er sich was unk du als nachstes unk weil es sich alles so unk als unk du nur die zeit unk oder unk ein wenig auf der unk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016</th>\n",
              "      <td>he unk it to provide a unk for his reform unk in unk</td>\n",
              "      <td>er erwartet davon auch einen unk fur sein unk in unk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>unk rights activists unk the decision the latest in a unk of unk against similar unk saying it would give doctors more time to unk hospital privileges</td>\n",
              "      <td>unk unk die entscheidung die unk in einer reihe von unk gegen unk manahmen und sagten dass arzte so mehr zeit unk um unk zu unk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447</th>\n",
              "      <td>unk are unk their anniversary</td>\n",
              "      <td>unk unk ihr unk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1100</th>\n",
              "      <td>i didnt think i was going to be able to drive all the way home</td>\n",
              "      <td>ich unk die unk nach hause nicht mehr zu schaffen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>i love being in a unk room</td>\n",
              "      <td>ich liebe es in einem unk zu sein</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1652</th>\n",
              "      <td>if the state unk to unk revenue unk in the coming year the law could unk the states business income tax rate as low as percent by</td>\n",
              "      <td>wenn der staat weiterhin seine unk im kommenden jahr erreicht konnte dieses gesetz den unk des staates bis bis auf prozent unk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2028</th>\n",
              "      <td>since she has been a unk of the unk of unk in which she unk as an expert on unk and european and international unk</td>\n",
              "      <td>seit ist sie unk der unk in der sie unk fur unk und unk und internationale unk ist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1727</th>\n",
              "      <td>its hard to unk twitch being unk into amazon unk video as unk as youtube could have just unk twitch</td>\n",
              "      <td>es ist schwer sich unk dass twitch so unk in amazon unk unk unk wird wie youtube twitch einfach hatte unk konnen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1256</th>\n",
              "      <td>dr unk unk unk that asbestos unk in the unk were unk unk unk the unk cause of death</td>\n",
              "      <td>dr unk unk sagt aus dass unk in der unk ohne jeden unk unk die unk sind</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1669</th>\n",
              "      <td>after that the official from guangzhou unk unk with unk unk with red wine which was unk with unk</td>\n",
              "      <td>unk unk der unk aus guangzhou mit unk an allerdings mit unk der zu dem unk unk wurde</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>574</th>\n",
              "      <td>unk unk have meanwhile unk by a quarter year on year in unk most unk unk such as unk unk and unk according to unk unk unk</td>\n",
              "      <td>das unk in den unk unk unk wie unk unk und unk ist unk jahr fur jahr um ein viertel unk so der unk unk unk</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                      eng                                                                                                                                                                                                 ger\n",
              "1502                                                                                                                                                                                       unk unk wanted                                                                                                                                                                                         unk unk unk\n",
              "1744                                                                                                                                                                    and that goes for all generations                                                                                                                                                                  und das unk durch die generationen\n",
              "669                                                                                                                                                                                       im unk that way                                                                                                                                                                         ich bin da ein bisschen unk\n",
              "362   unk unk tv series and parts in films unk his next unk but after his bbc series unk he thought what are you going to do next unk because it all unk a bit like youre unk time or youre unk going unk  unk unk unk und unk unk sein nachstes unk aber nach seiner unk unk dachte er sich was unk du als nachstes unk weil es sich alles so unk als unk du nur die zeit unk oder unk ein wenig auf der unk\n",
              "2016                                                                                                                                                 he unk it to provide a unk for his reform unk in unk                                                                                                                                                er erwartet davon auch einen unk fur sein unk in unk\n",
              "53                                                 unk rights activists unk the decision the latest in a unk of unk against similar unk saying it would give doctors more time to unk hospital privileges                                                                     unk unk die entscheidung die unk in einer reihe von unk gegen unk manahmen und sagten dass arzte so mehr zeit unk um unk zu unk\n",
              "1447                                                                                                                                                                        unk are unk their anniversary                                                                                                                                                                                     unk unk ihr unk\n",
              "1100                                                                                                                                       i didnt think i was going to be able to drive all the way home                                                                                                                                                   ich unk die unk nach hause nicht mehr zu schaffen\n",
              "395                                                                                                                                                                            i love being in a unk room                                                                                                                                                                   ich liebe es in einem unk zu sein\n",
              "1652                                                                    if the state unk to unk revenue unk in the coming year the law could unk the states business income tax rate as low as percent by                                                                      wenn der staat weiterhin seine unk im kommenden jahr erreicht konnte dieses gesetz den unk des staates bis bis auf prozent unk\n",
              "2028                                                                                   since she has been a unk of the unk of unk in which she unk as an expert on unk and european and international unk                                                                                                                  seit ist sie unk der unk in der sie unk fur unk und unk und internationale unk ist\n",
              "1727                                                                                                  its hard to unk twitch being unk into amazon unk video as unk as youtube could have just unk twitch                                                                                    es ist schwer sich unk dass twitch so unk in amazon unk unk unk wird wie youtube twitch einfach hatte unk konnen\n",
              "1256                                                                                                                  dr unk unk unk that asbestos unk in the unk were unk unk unk the unk cause of death                                                                                                                             dr unk unk sagt aus dass unk in der unk ohne jeden unk unk die unk sind\n",
              "1669                                                                                                     after that the official from guangzhou unk unk with unk unk with red wine which was unk with unk                                                                                                                unk unk der unk aus guangzhou mit unk an allerdings mit unk der zu dem unk unk wurde\n",
              "574                                                                             unk unk have meanwhile unk by a quarter year on year in unk most unk unk such as unk unk and unk according to unk unk unk                                                                                          das unk in den unk unk unk wie unk unk und unk ist unk jahr fur jahr um ein viertel unk so der unk unk unk"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y11hmIvPFXQi",
        "colab": {}
      },
      "source": [
        "#lines = lines[:5000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y4sjvE5Nwdsk"
      },
      "source": [
        "# 3)- Quick Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GEvO1bfy-KKP",
        "colab": {}
      },
      "source": [
        "# Lowercase all characters\n",
        "lines.eng=lines.eng.apply(lambda x: x.lower())\n",
        "lines.ger=lines.ger.apply(lambda x: x.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9bLMfF59-KNX",
        "colab": {}
      },
      "source": [
        "# Remove quotes\n",
        "lines.eng=lines.eng.apply(lambda x: re.sub(\"'\", '', x))\n",
        "lines.ger=lines.ger.apply(lambda x: re.sub(\"'\", '', x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t-rYHNHd-KQK",
        "colab": {}
      },
      "source": [
        "exclude = set(string.punctuation) # Set of all special characters\n",
        "# Remove all the special characters\n",
        "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "lines.ger=lines.ger.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bWGhz8JG-KVi",
        "colab": {}
      },
      "source": [
        "# Remove all numbers from text\n",
        "remove_digits = str.maketrans('', '', digits)\n",
        "lines.eng=lines.eng.apply(lambda x: x.translate(remove_digits))\n",
        "lines.ger=lines.ger.apply(lambda x: x.translate(remove_digits))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qCJnKqIY-KYm",
        "colab": {}
      },
      "source": [
        "# Remove extra spaces\n",
        "lines.eng=lines.eng.apply(lambda x: x.strip())\n",
        "lines.ger=lines.ger.apply(lambda x: x.strip())\n",
        "lines.eng=lines.eng.apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "lines.ger=lines.ger.apply(lambda x: re.sub(\" +\", \" \", x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-TJC18LaS_Qi"
      },
      "source": [
        "**Do threshold here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9AwSMz4O-pKQ",
        "colab": {}
      },
      "source": [
        "# Add start and end tokens to target sequences. I am not German so , I am doing so for my ease\n",
        "lines.ger = lines.ger.apply(lambda x : 'START_ '+ x + ' _END')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O-6Rm08s-yn9",
        "outputId": "9ef8970f-7e36-4e0a-dbb8-bae37c561057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "lines.ger[:5]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                                                                 START_ die premierminister unk und unk unk sich in unk _END\n",
              "1    START_ unk unk premierminister unk unk unk bei seinem ersten unk unk seit seinem unk im mai seinen unk unk unk unk in unk um unk und unk unk zu unk _END\n",
              "2                                        START_ herr unk unk sich auf einer unk unk nach unk um die wirtschaftlichen unk mit der unk unk der welt zu unk _END\n",
              "3                                                                                         START_ plane fur eine unk unk unk stehen ganz oben auf der unk _END\n",
              "4                                                        START_ berichten zufolge hofft unk daruber hinaus auf einen unk zur unk zwischen den beiden unk _END\n",
              "Name: ger, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5slsI5P7-Ja-",
        "outputId": "16421f29-809e-4f5a-e220-a31d5cefbc77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "lines.sample(10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>ger</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1394</th>\n",
              "      <td>the unk in unk presented unk and unk of the unk of as well as unk made from the unk unk unk unk which was once used to unk up unk</td>\n",
              "      <td>START_ die unk in unk unk unk und unk von den unk unk sowie unk aus der unk der unk die einst die unk unk _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1848</th>\n",
              "      <td>calculus is selling for used on unk</td>\n",
              "      <td>START_ unk ist unk auf unk fur zu haben _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>unk the unk road must be unk into two unk over a unk of about one unk</td>\n",
              "      <td>START_ damit musste die unk strae auf etwa einem kilometer lange unk unk werden _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>in the office of unk unk unk at the state association in unk unk the remains of a roman unk in an industrial estate in north unk</td>\n",
              "      <td>START_ hatte das unk fur unk beim unk unk die unk einer romischen unk im unk unk in unk unk _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>975</th>\n",
              "      <td>they are a big happy family now and alyona and unk are both great kids said a close unk</td>\n",
              "      <td>START_ sie sind jetzt eine groe unk familie und aljona und unk sind unk unk kinder sagt ein unk unk _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>930</th>\n",
              "      <td>in unk of unk condition i am well</td>\n",
              "      <td>START_ unk geht es mir gut _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>unk riders are unk in unk with unk riders being put up in unk unk</td>\n",
              "      <td>START_ in unk unk werden die teilnehmer in unk unk in unk ii in unk _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1624</th>\n",
              "      <td>a unk economy means more unk higher sales and new unk</td>\n",
              "      <td>START_ eine erholung der wirtschaft bedeutet mehr unk mehr unk und neue unk _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1860</th>\n",
              "      <td>unk unk if you unk to return the book unk can actually unk the cost of buying it new</td>\n",
              "      <td>START_ unk unk wenn sie das buch nicht unk konnen die unk den preis eines neuen unk sogar unk _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>and wouldnt it be unk if we could set our home unk to unk on just before we get back from the office</td>\n",
              "      <td>START_ und ware es nicht unk wenn wir vom buro aus die unk auf unsere unk unk _END</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                    eng                                                                                                             ger\n",
              "1394  the unk in unk presented unk and unk of the unk of as well as unk made from the unk unk unk unk which was once used to unk up unk  START_ die unk in unk unk unk und unk von den unk unk sowie unk aus der unk der unk die einst die unk unk _END\n",
              "1848                                                                                                calculus is selling for used on unk                                                                    START_ unk ist unk auf unk fur zu haben _END\n",
              "239                                                               unk the unk road must be unk into two unk over a unk of about one unk                            START_ damit musste die unk strae auf etwa einem kilometer lange unk unk werden _END\n",
              "1396   in the office of unk unk unk at the state association in unk unk the remains of a roman unk in an industrial estate in north unk                START_ hatte das unk fur unk beim unk unk die unk einer romischen unk im unk unk in unk unk _END\n",
              "975                                             they are a big happy family now and alyona and unk are both great kids said a close unk        START_ sie sind jetzt eine groe unk familie und aljona und unk sind unk unk kinder sagt ein unk unk _END\n",
              "930                                                                                                   in unk of unk condition i am well                                                                                 START_ unk geht es mir gut _END\n",
              "490                                                                   unk riders are unk in unk with unk riders being put up in unk unk                                        START_ in unk unk werden die teilnehmer in unk unk in unk ii in unk _END\n",
              "1624                                                                              a unk economy means more unk higher sales and new unk                                START_ eine erholung der wirtschaft bedeutet mehr unk mehr unk und neue unk _END\n",
              "1860                                               unk unk if you unk to return the book unk can actually unk the cost of buying it new              START_ unk unk wenn sie das buch nicht unk konnen die unk den preis eines neuen unk sogar unk _END\n",
              "158                                and wouldnt it be unk if we could set our home unk to unk on just before we get back from the office                              START_ und ware es nicht unk wenn wir vom buro aus die unk auf unsere unk unk _END"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D1HIfLfoMlYD"
      },
      "source": [
        "### 3a)- Vocab Size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tum2mSmK-Jeq",
        "colab": {}
      },
      "source": [
        "# Vocabulary of English\n",
        "all_eng_words=set()\n",
        "for eng in lines.eng:\n",
        "    for word in eng.split():\n",
        "        if word not in all_eng_words:\n",
        "            all_eng_words.add(word)\n",
        "# Vocabulary of German \n",
        "all_german_words=set()\n",
        "for ger in lines.ger:\n",
        "    for word in ger.split():\n",
        "        if word not in all_german_words:\n",
        "            all_german_words.add(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gGrnrWLw-Jhr",
        "outputId": "88140246-59ee-4d32-a7fa-4c58ecd71477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Max Length of source sequence\n",
        "import numpy as np\n",
        "lenght_list=[]\n",
        "for l in lines.eng:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_length_src = np.max(lenght_list)\n",
        "max_length_src"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NKYTzcsw-Jls",
        "outputId": "fafb2f0c-1055-4487-afba-608c6223ca56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Max Length of target sequence\n",
        "lenght_list=[]\n",
        "for l in lines.ger:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_length_tar = np.max(lenght_list)\n",
        "max_length_tar"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "njsfAZIhGmjf",
        "outputId": "77adeb56-c8e9-4ac8-a829-7c66d377217b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "lines['word_eng'] = lines['eng'].apply(lambda x: len(str(x).split(\" \")))\n",
        "lines[['eng','word_eng']].head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>word_eng</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>unk and unk prime unk unk in unk</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>unk new prime minister unk unk is meeting his unk unk unk unk in unk to unk economic and security unk on his first major foreign unk since unk unk election</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mr unk is on a unk unk to unk to unk economic unk with the third largest economy in the world</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>high on the unk are plans for unk unk unk</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>unk is also unk unk for a deal on unk unk between the two unk</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                           eng  word_eng\n",
              "0                                                                                                                             unk and unk prime unk unk in unk         8\n",
              "1  unk new prime minister unk unk is meeting his unk unk unk unk in unk to unk economic and security unk on his first major foreign unk since unk unk election        31\n",
              "2                                                                mr unk is on a unk unk to unk to unk economic unk with the third largest economy in the world        21\n",
              "3                                                                                                                    high on the unk are plans for unk unk unk        10\n",
              "4                                                                                                unk is also unk unk for a deal on unk unk between the two unk        15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j2cAOXePGmmg",
        "outputId": "b47333c6-7486-43c8-e58c-49710d260521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# adding +2 due to adding of START and END\n",
        "lines['word_ger'] = lines['ger'].apply(lambda x: len(str(x).split(\" \")))\n",
        "lines[['ger','word_ger']].head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ger</th>\n",
              "      <th>word_ger</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>START_ die premierminister unk und unk unk sich in unk _END</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>START_ unk unk premierminister unk unk unk bei seinem ersten unk unk seit seinem unk im mai seinen unk unk unk unk in unk um unk und unk unk zu unk _END</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>START_ herr unk unk sich auf einer unk unk nach unk um die wirtschaftlichen unk mit der unk unk der welt zu unk _END</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>START_ plane fur eine unk unk unk stehen ganz oben auf der unk _END</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>START_ berichten zufolge hofft unk daruber hinaus auf einen unk zur unk zwischen den beiden unk _END</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                        ger  word_ger\n",
              "0                                                                                               START_ die premierminister unk und unk unk sich in unk _END        11\n",
              "1  START_ unk unk premierminister unk unk unk bei seinem ersten unk unk seit seinem unk im mai seinen unk unk unk unk in unk um unk und unk unk zu unk _END        32\n",
              "2                                      START_ herr unk unk sich auf einer unk unk nach unk um die wirtschaftlichen unk mit der unk unk der welt zu unk _END        24\n",
              "3                                                                                       START_ plane fur eine unk unk unk stehen ganz oben auf der unk _END        14\n",
              "4                                                      START_ berichten zufolge hofft unk daruber hinaus auf einen unk zur unk zwischen den beiden unk _END        17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iAFqlcXjHy-F",
        "outputId": "f45f8b39-e8eb-4518-90e6-64989e8859fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "words_eng=lines['word_eng']\n",
        "plt.hist(words_eng, bins=20, label=\"Word count of english words\") \n",
        "plt.legend() \n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiKB+//Si4jIixrs5AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t3NjOtTIGmpO",
        "outputId": "9eb00a3e-0322-4fd4-bce6-4a88e0c3824f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "words_ger=lines['word_ger']\n",
        "plt.hist(words_ger, bins=20, label=\"Word count of German words\") \n",
        "plt.legend() \n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uF2tPX2JMgvN"
      },
      "source": [
        "### 3b)-Words' frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jfq_DzpnLXeK",
        "outputId": "eacebe5d-c89f-49e1-87e2-82c455303b88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# let’s check the 10 most frequently occurring words in our English text data\n",
        "freq_eng = pd.Series(' '.join(lines['eng']).split()).value_counts()[:10]\n",
        "freq_eng"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "unk     9455\n",
              "the     2643\n",
              "to      1115\n",
              "of      1078\n",
              "in      1015\n",
              "and      953\n",
              "a        903\n",
              "for      423\n",
              "that     410\n",
              "is       400\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CRqExh5_LKIC",
        "outputId": "295f6153-c86d-472d-8df4-52d65594b379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "freq_ger = pd.Series(' '.join(lines['ger']).split()).value_counts()[:12]\n",
        "freq_ger"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "unk       11732\n",
              "START_     2169\n",
              "_END       2169\n",
              "die        1256\n",
              "der        1177\n",
              "und         940\n",
              "in          790\n",
              "den         453\n",
              "von         433\n",
              "das         426\n",
              "zu          406\n",
              "fur         333\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S2jWXT3yM71x"
      },
      "source": [
        "As we have start and end so, I did check 12 so that we get idea of most fequent words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RDxvf8EbNCQI"
      },
      "source": [
        "**How about least occuring words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nw37EWeeNFTc",
        "outputId": "408e1552-eb8a-41de-efd7-513817450af3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "less_freq_eng = pd.Series(' '.join(lines['eng']).split()).value_counts()[-10:]\n",
        "less_freq_eng"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "founded         5\n",
              "spread          5\n",
              "rapid           5\n",
              "spent           5\n",
              "nice            5\n",
              "calculus        5\n",
              "intelligence    5\n",
              "remembers       5\n",
              "goods           5\n",
              "points          5\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aJqyRkyRNQno",
        "outputId": "21e965a6-2c9a-44cf-9985-0dd180037401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "less_freq_ger = pd.Series(' '.join(lines['ger']).split()).value_counts()[-10:]\n",
        "less_freq_ger"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "deal                   5\n",
              "stoff                  5\n",
              "allem                  5\n",
              "unternehmenssteuern    5\n",
              "gewerkschaft           5\n",
              "generationen           5\n",
              "november               5\n",
              "gleichen               5\n",
              "geismar                5\n",
              "kampfe                 5\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dLxosVx4wds5"
      },
      "source": [
        "### 3a)- Defining input and target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z8TUTnn4AC9c",
        "outputId": "0a71e319-b2aa-4310-c7d5-2cd55cc8093c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_words = sorted(list(all_eng_words))\n",
        "target_words = sorted(list(all_german_words))\n",
        "num_encoder_tokens = len(all_eng_words)\n",
        "num_decoder_tokens = len(all_german_words)\n",
        "num_encoder_tokens, num_decoder_tokens"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1211, 991)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G06K661ZADAo",
        "outputId": "403ec85b-7541-4aec-849a-3803dd45d8fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_decoder_tokens += 1 # For zero padding\n",
        "num_decoder_tokens"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "992"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pw2TwZh6ADGE",
        "colab": {}
      },
      "source": [
        "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3_DwM1HCADJk",
        "colab": {}
      },
      "source": [
        "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jfgAU6cH_g-L",
        "outputId": "2a9c79e5-486e-4068-c843-1946e230c7be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(reverse_input_char_index)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'a', 2: 'abandoned', 3: 'abbott', 4: 'able', 5: 'abortions', 6: 'about', 7: 'above', 8: 'access', 9: 'according', 10: 'account', 11: 'accused', 12: 'across', 13: 'act', 14: 'action', 15: 'actions', 16: 'activists', 17: 'actually', 18: 'addition', 19: 'adelaide', 20: 'admitting', 21: 'affected', 22: 'africa', 23: 'after', 24: 'again', 25: 'against', 26: 'age', 27: 'ago', 28: 'agreed', 29: 'agreement', 30: 'ahead', 31: 'ailinn', 32: 'air', 33: 'airport', 34: 'all', 35: 'allow', 36: 'almost', 37: 'alone', 38: 'along', 39: 'already', 40: 'also', 41: 'although', 42: 'always', 43: 'alyona', 44: 'am', 45: 'amazon', 46: 'america', 47: 'american', 48: 'americans', 49: 'among', 50: 'amount', 51: 'an', 52: 'and', 53: 'andrew', 54: 'anniversary', 55: 'announced', 56: 'announcement', 57: 'annual', 58: 'another', 59: 'answer', 60: 'any', 61: 'anyone', 62: 'apollo', 63: 'april', 64: 'are', 65: 'area', 66: 'areas', 67: 'arm', 68: 'army', 69: 'around', 70: 'as', 71: 'asbestos', 72: 'asked', 73: 'association', 74: 'at', 75: 'august', 76: 'authorities', 77: 'authority', 78: 'available', 79: 'average', 80: 'away', 81: 'back', 82: 'bad', 83: 'ballads', 84: 'band', 85: 'bank', 86: 'banned', 87: 'bar', 88: 'barisic', 89: 'based', 90: 'bbc', 91: 'be', 92: 'became', 93: 'because', 94: 'become', 95: 'becoming', 96: 'been', 97: 'beer', 98: 'before', 99: 'began', 100: 'beginning', 101: 'behind', 102: 'beijing', 103: 'being', 104: 'believe', 105: 'believed', 106: 'berngau', 107: 'beslan', 108: 'best', 109: 'better', 110: 'between', 111: 'big', 112: 'bike', 113: 'billion', 114: 'bills', 115: 'bit', 116: 'black', 117: 'blue', 118: 'body', 119: 'book', 120: 'books', 121: 'border', 122: 'born', 123: 'both', 124: 'box', 125: 'boy', 126: 'break', 127: 'britain', 128: 'british', 129: 'brought', 130: 'brown', 131: 'brussels', 132: 'building', 133: 'buildings', 134: 'business', 135: 'businesses', 136: 'but', 137: 'buying', 138: 'by', 139: 'calculus', 140: 'call', 141: 'called', 142: 'came', 143: 'cameras', 144: 'campaign', 145: 'can', 146: 'cancer', 147: 'candidates', 148: 'cannon', 149: 'cant', 150: 'capital', 151: 'car', 152: 'career', 153: 'carried', 154: 'cars', 155: 'case', 156: 'castle', 157: 'cause', 158: 'caused', 159: 'cdu', 160: 'cent', 161: 'center', 162: 'central', 163: 'centre', 164: 'certainly', 165: 'chairman', 166: 'challenge', 167: 'change', 168: 'changed', 169: 'charges', 170: 'chemotherapy', 171: 'chest', 172: 'chief', 173: 'child', 174: 'children', 175: 'childrens', 176: 'china', 177: 'chinas', 178: 'chinese', 179: 'cities', 180: 'city', 181: 'civil', 182: 'claim', 183: 'claimed', 184: 'claims', 185: 'class', 186: 'clear', 187: 'clinics', 188: 'close', 189: 'closed', 190: 'closer', 191: 'club', 192: 'clubs', 193: 'cmt', 194: 'college', 195: 'color', 196: 'come', 197: 'comedy', 198: 'comes', 199: 'coming', 200: 'comment', 201: 'commission', 202: 'committee', 203: 'communities', 204: 'community', 205: 'companies', 206: 'company', 207: 'compared', 208: 'compensation', 209: 'complete', 210: 'completely', 211: 'concern', 212: 'condition', 213: 'conditions', 214: 'confirmed', 215: 'conflict', 216: 'connection', 217: 'consider', 218: 'considering', 219: 'constitutional', 220: 'continue', 221: 'continued', 222: 'control', 223: 'conversation', 224: 'corporate', 225: 'corruption', 226: 'cost', 227: 'could', 228: 'couldnt', 229: 'council', 230: 'countries', 231: 'country', 232: 'county', 233: 'course', 234: 'court', 235: 'crisis', 236: 'criticism', 237: 'cruise', 238: 'cruises', 239: 'csu', 240: 'current', 241: 'cycling', 242: 'cyclists', 243: 'dad', 244: 'daily', 245: 'damir', 246: 'dance', 247: 'dangerous', 248: 'data', 249: 'daughter', 250: 'day', 251: 'days', 252: 'dead', 253: 'deal', 254: 'death', 255: 'decades', 256: 'december', 257: 'decided', 258: 'decision', 259: 'delayed', 260: 'demanding', 261: 'democracy', 262: 'democratic', 263: 'democrats', 264: 'demonstrators', 265: 'denied', 266: 'deputy', 267: 'described', 268: 'describes', 269: 'despite', 270: 'development', 271: 'diagnosed', 272: 'diagnosis', 273: 'did', 274: 'didnt', 275: 'die', 276: 'died', 277: 'different', 278: 'difficult', 279: 'dimbath', 280: 'director', 281: 'discrimination', 282: 'disease', 283: 'district', 284: 'division', 285: 'do', 286: 'doctor', 287: 'doctors', 288: 'does', 289: 'doing', 290: 'done', 291: 'dont', 292: 'down', 293: 'dr', 294: 'drive', 295: 'drop', 296: 'due', 297: 'during', 298: 'dying', 299: 'each', 300: 'earlier', 301: 'early', 302: 'east', 303: 'eastern', 304: 'ebola', 305: 'economic', 306: 'economy', 307: 'effect', 308: 'either', 309: 'election', 310: 'elections', 311: 'else', 312: 'employees', 313: 'end', 314: 'energy', 315: 'enough', 316: 'ensure', 317: 'entertainment', 318: 'epidemic', 319: 'esme', 320: 'especially', 321: 'estate', 322: 'eu', 323: 'europe', 324: 'european', 325: 'euros', 326: 'eurozone', 327: 'even', 328: 'evening', 329: 'event', 330: 'events', 331: 'eventually', 332: 'ever', 333: 'every', 334: 'everyone', 335: 'everything', 336: 'exactly', 337: 'example', 338: 'executive', 339: 'exercise', 340: 'expected', 341: 'experience', 342: 'expert', 343: 'explained', 344: 'exposed', 345: 'exposure', 346: 'eyes', 347: 'face', 348: 'facebook', 349: 'fact', 350: 'failed', 351: 'fall', 352: 'families', 353: 'family', 354: 'far', 355: 'father', 356: 'fear', 357: 'federal', 358: 'feel', 359: 'feeling', 360: 'feet', 361: 'feldberg', 362: 'fell', 363: 'fellow', 364: 'felt', 365: 'feminism', 366: 'feminist', 367: 'feminists', 368: 'ferguson', 369: 'festival', 370: 'few', 371: 'fibres', 372: 'fifth', 373: 'fight', 374: 'fighting', 375: 'figures', 376: 'film', 377: 'films', 378: 'financial', 379: 'find', 380: 'fingers', 381: 'fire', 382: 'firm', 383: 'first', 384: 'five', 385: 'flight', 386: 'followed', 387: 'following', 388: 'food', 389: 'foot', 390: 'for', 391: 'force', 392: 'forced', 393: 'forces', 394: 'foreign', 395: 'forget', 396: 'form', 397: 'former', 398: 'forward', 399: 'found', 400: 'founded', 401: 'four', 402: 'frankfurt', 403: 'french', 404: 'frequently', 405: 'friday', 406: 'friends', 407: 'from', 408: 'front', 409: 'full', 410: 'fun', 411: 'further', 412: 'future', 413: 'game', 414: 'games', 415: 'garden', 416: 'gas', 417: 'gave', 418: 'gaza', 419: 'geismar', 420: 'gender', 421: 'general', 422: 'generations', 423: 'georgy', 424: 'german', 425: 'germany', 426: 'get', 427: 'getting', 428: 'giant', 429: 'girl', 430: 'give', 431: 'given', 432: 'giving', 433: 'glass', 434: 'global', 435: 'go', 436: 'goal', 437: 'goes', 438: 'going', 439: 'good', 440: 'goods', 441: 'got', 442: 'government', 443: 'governments', 444: 'graham', 445: 'great', 446: 'greatest', 447: 'grecko', 448: 'ground', 449: 'group', 450: 'grow', 451: 'growth', 452: 'guangzhou', 453: 'gun', 454: 'guys', 455: 'gym', 456: 'hacker', 457: 'had', 458: 'hadnt', 459: 'half', 460: 'hall', 461: 'hand', 462: 'hands', 463: 'happen', 464: 'happened', 465: 'happy', 466: 'hard', 467: 'has', 468: 'have', 469: 'having', 470: 'he', 471: 'head', 472: 'health', 473: 'heard', 474: 'heart', 475: 'heavy', 476: 'held', 477: 'hell', 478: 'help', 479: 'henry', 480: 'her', 481: 'here', 482: 'high', 483: 'higher', 484: 'him', 485: 'himself', 486: 'his', 487: 'history', 488: 'hit', 489: 'hofmann', 490: 'hold', 491: 'home', 492: 'homes', 493: 'honest', 494: 'hong', 495: 'hope', 496: 'hopes', 497: 'horror', 498: 'hospital', 499: 'hospitals', 500: 'hotel', 501: 'hours', 502: 'house', 503: 'housing', 504: 'houthi', 505: 'how', 506: 'however', 507: 'huge', 508: 'human', 509: 'hundreds', 510: 'i', 511: 'id', 512: 'idea', 513: 'if', 514: 'ii', 515: 'im', 516: 'image', 517: 'images', 518: 'immediately', 519: 'important', 520: 'impression', 521: 'in', 522: 'included', 523: 'including', 524: 'income', 525: 'industrial', 526: 'industry', 527: 'information', 528: 'injured', 529: 'inside', 530: 'instead', 531: 'intelligence', 532: 'interested', 533: 'interests', 534: 'international', 535: 'internet', 536: 'into', 537: 'is', 538: 'israeli', 539: 'issue', 540: 'issues', 541: 'it', 542: 'its', 543: 'itself', 544: 'jazz', 545: 'jennifer', 546: 'job', 547: 'judge', 548: 'july', 549: 'june', 550: 'just', 551: 'justice', 552: 'keep', 553: 'keeping', 554: 'kept', 555: 'kevern', 556: 'key', 557: 'khan', 558: 'kids', 559: 'kiev', 560: 'killed', 561: 'kind', 562: 'knew', 563: 'know', 564: 'known', 565: 'kong', 566: 'kongs', 567: 'kremlin', 568: 'labor', 569: 'large', 570: 'largest', 571: 'last', 572: 'late', 573: 'later', 574: 'latest', 575: 'launched', 576: 'law', 577: 'lawrence', 578: 'lawson', 579: 'lead', 580: 'leader', 581: 'leaders', 582: 'league', 583: 'least', 584: 'leave', 585: 'led', 586: 'left', 587: 'legal', 588: 'legislation', 589: 'lesotho', 590: 'less', 591: 'let', 592: 'level', 593: 'levels', 594: 'li', 595: 'life', 596: 'light', 597: 'like', 598: 'likely', 599: 'line', 600: 'lining', 601: 'list', 602: 'little', 603: 'live', 604: 'lives', 605: 'living', 606: 'local', 607: 'located', 608: 'london', 609: 'long', 610: 'longer', 611: 'look', 612: 'looked', 613: 'looking', 614: 'lose', 615: 'lost', 616: 'lot', 617: 'louisiana', 618: 'love', 619: 'low', 620: 'lower', 621: 'lung', 622: 'made', 623: 'magaluf', 624: 'main', 625: 'major', 626: 'majority', 627: 'make', 628: 'makes', 629: 'making', 630: 'man', 631: 'managed', 632: 'management', 633: 'manager', 634: 'manfred', 635: 'many', 636: 'march', 637: 'market', 638: 'markets', 639: 'maryland', 640: 'matter', 641: 'may', 642: 'maybe', 643: 'mayor', 644: 'me', 645: 'meaning', 646: 'means', 647: 'meant', 648: 'meanwhile', 649: 'medau', 650: 'media', 651: 'medical', 652: 'meeting', 653: 'members', 654: 'memory', 655: 'men', 656: 'mesothelioma', 657: 'meters', 658: 'michael', 659: 'middle', 660: 'might', 661: 'military', 662: 'million', 663: 'minister', 664: 'ministry', 665: 'minutes', 666: 'mitchell', 667: 'mogherini', 668: 'moment', 669: 'monday', 670: 'money', 671: 'month', 672: 'months', 673: 'more', 674: 'morgan', 675: 'morning', 676: 'moscow', 677: 'most', 678: 'mother', 679: 'motorcycle', 680: 'move', 681: 'moved', 682: 'movement', 683: 'mr', 684: 'much', 685: 'mum', 686: 'music', 687: 'must', 688: 'my', 689: 'myself', 690: 'name', 691: 'national', 692: 'nato', 693: 'natural', 694: 'near', 695: 'nearly', 696: 'necessary', 697: 'need', 698: 'needed', 699: 'needs', 700: 'never', 701: 'new', 702: 'news', 703: 'newspaper', 704: 'next', 705: 'nice', 706: 'night', 707: 'nine', 708: 'no', 709: 'normal', 710: 'north', 711: 'norwegian', 712: 'not', 713: 'noted', 714: 'notes', 715: 'nothing', 716: 'november', 717: 'now', 718: 'npd', 719: 'nude', 720: 'number', 721: 'nussbaum', 722: 'of', 723: 'off', 724: 'offered', 725: 'office', 726: 'officers', 727: 'official', 728: 'officials', 729: 'often', 730: 'old', 731: 'older', 732: 'on', 733: 'once', 734: 'one', 735: 'ongoing', 736: 'online', 737: 'only', 738: 'open', 739: 'opportunity', 740: 'opposition', 741: 'options', 742: 'or', 743: 'order', 744: 'organization', 745: 'organize', 746: 'organized', 747: 'organizers', 748: 'organizing', 749: 'other', 750: 'others', 751: 'our', 752: 'out', 753: 'outside', 754: 'ovarian', 755: 'over', 756: 'own', 757: 'owners', 758: 'paid', 759: 'pain', 760: 'park', 761: 'parliament', 762: 'parliamentary', 763: 'part', 764: 'participants', 765: 'particularly', 766: 'partner', 767: 'parts', 768: 'party', 769: 'past', 770: 'patients', 771: 'pay', 772: 'pd', 773: 'peaceful', 774: 'people', 775: 'peoples', 776: 'per', 777: 'percent', 778: 'period', 779: 'perry', 780: 'person', 781: 'personal', 782: 'photos', 783: 'picture', 784: 'piffl', 785: 'pig', 786: 'place', 787: 'places', 788: 'plan', 789: 'plans', 790: 'play', 791: 'played', 792: 'playing', 793: 'pleura', 794: 'point', 795: 'points', 796: 'police', 797: 'political', 798: 'politician', 799: 'popular', 800: 'position', 801: 'positive', 802: 'possible', 803: 'post', 804: 'posted', 805: 'potential', 806: 'power', 807: 'premier', 808: 'presented', 809: 'president', 810: 'prestige', 811: 'price', 812: 'prices', 813: 'prime', 814: 'prison', 815: 'private', 816: 'privileges', 817: 'problem', 818: 'problems', 819: 'process', 820: 'professor', 821: 'program', 822: 'project', 823: 'promised', 824: 'property', 825: 'protect', 826: 'protest', 827: 'protests', 828: 'prove', 829: 'provide', 830: 'pub', 831: 'public', 832: 'put', 833: 'putin', 834: 'qadri', 835: 'quarter', 836: 'quickly', 837: 'quite', 838: 'race', 839: 'radio', 840: 'rain', 841: 'ran', 842: 'rapid', 843: 'rate', 844: 'rates', 845: 'rather', 846: 'reach', 847: 'reached', 848: 'ready', 849: 'real', 850: 'realised', 851: 'really', 852: 'reason', 853: 'rebels', 854: 'receive', 855: 'received', 856: 'recent', 857: 'recently', 858: 'reconstruction', 859: 'red', 860: 'reform', 861: 'refugees', 862: 'refused', 863: 'region', 864: 'regional', 865: 'regions', 866: 'relations', 867: 'remained', 868: 'remains', 869: 'remember', 870: 'remembers', 871: 'removed', 872: 'renting', 873: 'renzi', 874: 'report', 875: 'reported', 876: 'research', 877: 'residence', 878: 'resort', 879: 'result', 880: 'results', 881: 'return', 882: 'returned', 883: 'revenue', 884: 'richter', 885: 'riders', 886: 'right', 887: 'rights', 888: 'rise', 889: 'road', 890: 'role', 891: 'roman', 892: 'room', 893: 'rose', 894: 'round', 895: 'route', 896: 'ruling', 897: 'run', 898: 'russia', 899: 'russian', 900: 'safety', 901: 'said', 902: 'sale', 903: 'sales', 904: 'salzhausen', 905: 'salzwedel', 906: 'same', 907: 'saturday', 908: 'saw', 909: 'say', 910: 'saying', 911: 'says', 912: 'school', 913: 'schools', 914: 'schotten', 915: 'schrammel', 916: 'season', 917: 'second', 918: 'secretary', 919: 'security', 920: 'see', 921: 'seeking', 922: 'seemed', 923: 'seems', 924: 'seen', 925: 'seized', 926: 'selling', 927: 'semester', 928: 'senior', 929: 'sense', 930: 'sent', 931: 'separatists', 932: 'september', 933: 'series', 934: 'servants', 935: 'service', 936: 'services', 937: 'set', 938: 'seven', 939: 'several', 940: 'shall', 941: 'share', 942: 'sharif', 943: 'she', 944: 'ships', 945: 'shooting', 946: 'short', 947: 'shot', 948: 'should', 949: 'show', 950: 'showed', 951: 'shows', 952: 'side', 953: 'siege', 954: 'sign', 955: 'silence', 956: 'similar', 957: 'simply', 958: 'since', 959: 'sitting', 960: 'six', 961: 'size', 962: 'small', 963: 'so', 964: 'social', 965: 'socialist', 966: 'society', 967: 'soldiers', 968: 'some', 969: 'something', 970: 'sometimes', 971: 'somewhat', 972: 'son', 973: 'song', 974: 'soon', 975: 'sorry', 976: 'sort', 977: 'south', 978: 'southern', 979: 'speaking', 980: 'special', 981: 'speech', 982: 'spending', 983: 'spent', 984: 'spokesman', 985: 'spread', 986: 'spring', 987: 'stage', 988: 'stand', 989: 'standing', 990: 'star', 991: 'start', 992: 'started', 993: 'starting', 994: 'state', 995: 'statement', 996: 'states', 997: 'station', 998: 'status', 999: 'stay', 1000: 'stewart', 1001: 'still', 1002: 'stolen', 1003: 'stop', 1004: 'store', 1005: 'stories', 1006: 'story', 1007: 'strike', 1008: 'strong', 1009: 'structure', 1010: 'students', 1011: 'study', 1012: 'success', 1013: 'such', 1014: 'suffered', 1015: 'sufferers', 1016: 'suggest', 1017: 'suggested', 1018: 'summer', 1019: 'summit', 1020: 'sunday', 1021: 'support', 1022: 'sure', 1023: 'surgery', 1024: 'survival', 1025: 'survive', 1026: 'symptoms', 1027: 'table', 1028: 'take', 1029: 'taken', 1030: 'taking', 1031: 'talk', 1032: 'talking', 1033: 'talks', 1034: 'tannenwald', 1035: 'target', 1036: 'tax', 1037: 'taxes', 1038: 'teacher', 1039: 'team', 1040: 'teams', 1041: 'tell', 1042: 'ten', 1043: 'tents', 1044: 'territory', 1045: 'texas', 1046: 'textbooks', 1047: 'texts', 1048: 'than', 1049: 'that', 1050: 'thats', 1051: 'the', 1052: 'their', 1053: 'them', 1054: 'themselves', 1055: 'then', 1056: 'there', 1057: 'therefore', 1058: 'these', 1059: 'they', 1060: 'theyre', 1061: 'thick', 1062: 'thing', 1063: 'things', 1064: 'think', 1065: 'thinking', 1066: 'third', 1067: 'this', 1068: 'thomas', 1069: 'those', 1070: 'though', 1071: 'thought', 1072: 'thousands', 1073: 'threatening', 1074: 'three', 1075: 'through', 1076: 'throughout', 1077: 'thursday', 1078: 'time', 1079: 'times', 1080: 'to', 1081: 'today', 1082: 'todays', 1083: 'together', 1084: 'told', 1085: 'toll', 1086: 'tolls', 1087: 'too', 1088: 'took', 1089: 'top', 1090: 'torpedo', 1091: 'total', 1092: 'tour', 1093: 'town', 1094: 'trade', 1095: 'traffic', 1096: 'train', 1097: 'trainees', 1098: 'training', 1099: 'transport', 1100: 'treasure', 1101: 'treated', 1102: 'treatment', 1103: 'tree', 1104: 'trial', 1105: 'tried', 1106: 'troops', 1107: 'try', 1108: 'turkish', 1109: 'turn', 1110: 'turned', 1111: 'tv', 1112: 'tweet', 1113: 'twin', 1114: 'twitch', 1115: 'two', 1116: 'uk', 1117: 'ukraine', 1118: 'ukrainian', 1119: 'under', 1120: 'understand', 1121: 'understanding', 1122: 'underwater', 1123: 'union', 1124: 'united', 1125: 'university', 1126: 'unk', 1127: 'until', 1128: 'up', 1129: 'upper', 1130: 'us', 1131: 'use', 1132: 'used', 1133: 'usually', 1134: 'vacuum', 1135: 'values', 1136: 'very', 1137: 'via', 1138: 'video', 1139: 'view', 1140: 'violence', 1141: 'virginia', 1142: 'visitors', 1143: 'voice', 1144: 'vote', 1145: 'wachtberg', 1146: 'wall', 1147: 'want', 1148: 'wanted', 1149: 'wants', 1150: 'war', 1151: 'warned', 1152: 'was', 1153: 'wasnt', 1154: 'water', 1155: 'way', 1156: 'we', 1157: 'weather', 1158: 'week', 1159: 'weekend', 1160: 'weeks', 1161: 'weende', 1162: 'well', 1163: 'went', 1164: 'were', 1165: 'west', 1166: 'what', 1167: 'when', 1168: 'where', 1169: 'whether', 1170: 'which', 1171: 'while', 1172: 'white', 1173: 'who', 1174: 'whole', 1175: 'whom', 1176: 'whose', 1177: 'why', 1178: 'wife', 1179: 'will', 1180: 'win', 1181: 'wine', 1182: 'wish', 1183: 'with', 1184: 'within', 1185: 'without', 1186: 'wolfratshausen', 1187: 'woman', 1188: 'women', 1189: 'womens', 1190: 'won', 1191: 'wont', 1192: 'word', 1193: 'words', 1194: 'work', 1195: 'worked', 1196: 'workers', 1197: 'working', 1198: 'works', 1199: 'world', 1200: 'would', 1201: 'wouldnt', 1202: 'wrote', 1203: 'year', 1204: 'years', 1205: 'yesterday', 1206: 'yet', 1207: 'you', 1208: 'young', 1209: 'your', 1210: 'youre', 1211: 'youtube'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVLHBLON_kQY",
        "outputId": "df64db34-6536-4d8c-ad3b-24c4ecf49bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(reverse_target_char_index)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'START_', 2: '_END', 3: 'ab', 4: 'abbott', 5: 'abend', 6: 'aber', 7: 'abtreibungen', 8: 'adelaide', 9: 'ailinn', 10: 'aljona', 11: 'all', 12: 'alle', 13: 'allein', 14: 'allem', 15: 'allen', 16: 'aller', 17: 'allerdings', 18: 'alles', 19: 'als', 20: 'also', 21: 'alt', 22: 'alten', 23: 'alter', 24: 'altere', 25: 'am', 26: 'amazon', 27: 'amerikanischen', 28: 'an', 29: 'andere', 30: 'anderem', 31: 'anderen', 32: 'anders', 33: 'andrew', 34: 'anfang', 35: 'angaben', 36: 'angeblich', 37: 'angeboten', 38: 'angekundigt', 39: 'angst', 40: 'anklage', 41: 'anstieg', 42: 'anteil', 43: 'apollo', 44: 'april', 45: 'arbeit', 46: 'arbeiten', 47: 'arbeiter', 48: 'arbeitete', 49: 'arbeitsplatz', 50: 'armee', 51: 'art', 52: 'arzt', 53: 'arzte', 54: 'arzten', 55: 'asbest', 56: 'asbestkontakt', 57: 'auch', 58: 'auer', 59: 'auerdem', 60: 'auf', 61: 'aufgabe', 62: 'aufgenommen', 63: 'aufgerufen', 64: 'aufgrund', 65: 'aufhoren', 66: 'augen', 67: 'august', 68: 'aus', 69: 'ausbildung', 70: 'ausgaben', 71: 'ausgesetzt', 72: 'auslandische', 73: 'aussage', 74: 'auswirkungen', 75: 'bad', 76: 'bald', 77: 'barisic', 78: 'bbc', 79: 'bedeutet', 80: 'begann', 81: 'beginn', 82: 'behandlung', 83: 'behorden', 84: 'bei', 85: 'beide', 86: 'beiden', 87: 'beim', 88: 'beispiel', 89: 'beispielsweise', 90: 'bekannt', 91: 'belagerung', 92: 'bereit', 93: 'bereits', 94: 'bericht', 95: 'berichten', 96: 'berichtet', 97: 'berichtete', 98: 'beschrieb', 99: 'beschuldigt', 100: 'beslan', 101: 'besonders', 102: 'besser', 103: 'bestatigte', 104: 'besten', 105: 'besucher', 106: 'beteiligt', 107: 'betroffenen', 108: 'bevor', 109: 'bier', 110: 'bild', 111: 'bilder', 112: 'bin', 113: 'bis', 114: 'bisher', 115: 'bisschen', 116: 'bleiben', 117: 'bleibt', 118: 'blieb', 119: 'brachte', 120: 'brussel', 121: 'brustfell', 122: 'buch', 123: 'bucher', 124: 'burgermeister', 125: 'burgfreunde', 126: 'burgverein', 127: 'buro', 128: 'cdu', 129: 'chef', 130: 'china', 131: 'chinas', 132: 'chinesische', 133: 'cmt', 134: 'cruise', 135: 'cruises', 136: 'da', 137: 'dabei', 138: 'dachte', 139: 'dafur', 140: 'daher', 141: 'damals', 142: 'damir', 143: 'damit', 144: 'danach', 145: 'dankbar', 146: 'dann', 147: 'daran', 148: 'darauf', 149: 'darf', 150: 'daruber', 151: 'darum', 152: 'darunter', 153: 'das', 154: 'dass', 155: 'daten', 156: 'davon', 157: 'davor', 158: 'dazu', 159: 'deal', 160: 'deine', 161: 'dem', 162: 'demokratie', 163: 'demonstranten', 164: 'den', 165: 'denen', 166: 'denke', 167: 'denken', 168: 'denn', 169: 'dennoch', 170: 'der', 171: 'deren', 172: 'derzeit', 173: 'des', 174: 'deshalb', 175: 'dessen', 176: 'deutlich', 177: 'deutsche', 178: 'deutschen', 179: 'deutschland', 180: 'dezember', 181: 'diagnose', 182: 'diagnostiziert', 183: 'dich', 184: 'die', 185: 'diejenigen', 186: 'dies', 187: 'diese', 188: 'diesem', 189: 'diesen', 190: 'dieser', 191: 'dieses', 192: 'dinge', 193: 'direkt', 194: 'district', 195: 'doch', 196: 'dollar', 197: 'donnerstag', 198: 'dort', 199: 'dr', 200: 'drei', 201: 'du', 202: 'durch', 203: 'ebenfalls', 204: 'ebenso', 205: 'eher', 206: 'ehrlich', 207: 'eierstockkrebs', 208: 'eigene', 209: 'eigenen', 210: 'ein', 211: 'eindruck', 212: 'eine', 213: 'einem', 214: 'einen', 215: 'einer', 216: 'eines', 217: 'einfach', 218: 'einige', 219: 'einigen', 220: 'einmal', 221: 'einnahmen', 222: 'einst', 223: 'elf', 224: 'eltern', 225: 'ende', 226: 'entfernt', 227: 'entschadigung', 228: 'entscheidung', 229: 'entwicklung', 230: 'er', 231: 'erfahren', 232: 'erfahrung', 233: 'ergebnis', 234: 'ergebnisse', 235: 'erhalten', 236: 'erholung', 237: 'erinnere', 238: 'erinnern', 239: 'erinnert', 240: 'erklart', 241: 'erklarte', 242: 'ernst', 243: 'erreicht', 244: 'erst', 245: 'erste', 246: 'ersten', 247: 'erwartet', 248: 'erzahlt', 249: 'es', 250: 'esme', 251: 'etwa', 252: 'etwas', 253: 'eu', 254: 'euro', 255: 'eurozone', 256: 'fall', 257: 'familie', 258: 'familien', 259: 'fast', 260: 'feldberg', 261: 'feministinnen', 262: 'ferguson', 263: 'fest', 264: 'fiel', 265: 'finden', 266: 'firmen', 267: 'fluchtlinge', 268: 'flughafen', 269: 'folge', 270: 'fordern', 271: 'form', 272: 'foto', 273: 'fotos', 274: 'frage', 275: 'fragen', 276: 'frankfurt', 277: 'frankreich', 278: 'frau', 279: 'frauen', 280: 'freitag', 281: 'freunde', 282: 'fruher', 283: 'fruhere', 284: 'fruheren', 285: 'fue', 286: 'fugte', 287: 'fuhlen', 288: 'fuhlte', 289: 'fuhr', 290: 'fuhren', 291: 'fuhrt', 292: 'fuhrte', 293: 'fuhrung', 294: 'funf', 295: 'fur', 296: 'furcht', 297: 'gab', 298: 'ganz', 299: 'ganze', 300: 'ganzen', 301: 'gar', 302: 'garten', 303: 'gaza', 304: 'gearbeitet', 305: 'geben', 306: 'gebiet', 307: 'gebieten', 308: 'gebracht', 309: 'gefahren', 310: 'gefahrlich', 311: 'gefangen', 312: 'gefuhrt', 313: 'gefunden', 314: 'gegen', 315: 'gegensatz', 316: 'gegenuber', 317: 'gegrundet', 318: 'gehabt', 319: 'gehen', 320: 'gehort', 321: 'geht', 322: 'geismar', 323: 'geld', 324: 'gemacht', 325: 'gemeinsam', 326: 'genannt', 327: 'genau', 328: 'generationen', 329: 'genommen', 330: 'georgij', 331: 'gerade', 332: 'gericht', 333: 'gerne', 334: 'gesagt', 335: 'geschehen', 336: 'geschichte', 337: 'geschichten', 338: 'gesehen', 339: 'gesellschaft', 340: 'gesetz', 341: 'gespielt', 342: 'gesprach', 343: 'gesprache', 344: 'gestern', 345: 'gesundheit', 346: 'getan', 347: 'getotet', 348: 'getroffen', 349: 'gewalt', 350: 'gewerkschaft', 351: 'gewerkschaften', 352: 'gewesen', 353: 'geworden', 354: 'gezeigt', 355: 'gezwungen', 356: 'gibt', 357: 'ging', 358: 'glandorf', 359: 'glaube', 360: 'glauben', 361: 'gleichen', 362: 'gluck', 363: 'graham', 364: 'grecko', 365: 'gro', 366: 'grobritannien', 367: 'groe', 368: 'groen', 369: 'groer', 370: 'groeren', 371: 'grote', 372: 'groteil', 373: 'grund', 374: 'gruppe', 375: 'guangzhou', 376: 'gut', 377: 'guten', 378: 'gutes', 379: 'guys', 380: 'habe', 381: 'haben', 382: 'hacker', 383: 'halfte', 384: 'halten', 385: 'hamas', 386: 'hand', 387: 'hat', 388: 'hatte', 389: 'hatten', 390: 'haufig', 391: 'hauptstadt', 392: 'haus', 393: 'hause', 394: 'hauses', 395: 'haut', 396: 'helfen', 397: 'henry', 398: 'herr', 399: 'heute', 400: 'heutigen', 401: 'hielt', 402: 'hier', 403: 'hilfe', 404: 'himmel', 405: 'hin', 406: 'hinaus', 407: 'hinter', 408: 'hinzu', 409: 'hochschule', 410: 'hoffe', 411: 'hoffnung', 412: 'hofft', 413: 'hofmann', 414: 'hohe', 415: 'hohen', 416: 'hoher', 417: 'hongkong', 418: 'horen', 419: 'horror', 420: 'hospital', 421: 'hunderte', 422: 'ich', 423: 'ihm', 424: 'ihn', 425: 'ihnen', 426: 'ihr', 427: 'ihre', 428: 'ihrem', 429: 'ihren', 430: 'ihrer', 431: 'ihres', 432: 'ii', 433: 'im', 434: 'immer', 435: 'immobiliensteuern', 436: 'in', 437: 'indem', 438: 'informationen', 439: 'innerhalb', 440: 'ins', 441: 'insbesondere', 442: 'insgesamt', 443: 'internationale', 444: 'inzwischen', 445: 'ist', 446: 'ja', 447: 'jahr', 448: 'jahre', 449: 'jahren', 450: 'jahres', 451: 'jahrlich', 452: 'jazz', 453: 'je', 454: 'jeden', 455: 'jedoch', 456: 'jennifer', 457: 'jetzt', 458: 'juli', 459: 'junge', 460: 'jungen', 461: 'juni', 462: 'kam', 463: 'kamen', 464: 'kampf', 465: 'kampfe', 466: 'kampfen', 467: 'kandidaten', 468: 'kann', 469: 'karriere', 470: 'kaum', 471: 'kein', 472: 'keine', 473: 'keinen', 474: 'kevern', 475: 'khan', 476: 'kiefer', 477: 'kiew', 478: 'kilometer', 479: 'kinder', 480: 'kindern', 481: 'klar', 482: 'kleine', 483: 'kleiner', 484: 'kleines', 485: 'knapp', 486: 'kommen', 487: 'kommenden', 488: 'kommt', 489: 'konne', 490: 'konnen', 491: 'konnte', 492: 'konnten', 493: 'kontakt', 494: 'kontrolle', 495: 'kopf', 496: 'korper', 497: 'korruption', 498: 'kosten', 499: 'krankenhaus', 500: 'krankheit', 501: 'krieg', 502: 'krim', 503: 'krise', 504: 'kurz', 505: 'kurzlich', 506: 'lag', 507: 'land', 508: 'landern', 509: 'landes', 510: 'landlichen', 511: 'lang', 512: 'lange', 513: 'langer', 514: 'langsam', 515: 'lassen', 516: 'lasst', 517: 'laufen', 518: 'laut', 519: 'lawrence', 520: 'lawson', 521: 'leben', 522: 'lediglich', 523: 'legte', 524: 'leicht', 525: 'leiden', 526: 'lernen', 527: 'lesotho', 528: 'letzte', 529: 'letzten', 530: 'leute', 531: 'li', 532: 'licht', 533: 'lie', 534: 'liebe', 535: 'liegt', 536: 'london', 537: 'losung', 538: 'louisiana', 539: 'lungen', 540: 'machen', 541: 'macht', 542: 'machte', 543: 'machten', 544: 'madchen', 545: 'magaluf', 546: 'mai', 547: 'mal', 548: 'man', 549: 'manahmen', 550: 'manchmal', 551: 'manfred', 552: 'mann', 553: 'manner', 554: 'mannschaft', 555: 'mannschaften', 556: 'markt', 557: 'maryland', 558: 'marz', 559: 'mauer', 560: 'maut', 561: 'mehr', 562: 'mehrere', 563: 'mein', 564: 'meine', 565: 'meinem', 566: 'meinen', 567: 'meiner', 568: 'meinung', 569: 'meisten', 570: 'menschen', 571: 'mesotheliom', 572: 'meter', 573: 'mich', 574: 'michael', 575: 'mieten', 576: 'militar', 577: 'milliarden', 578: 'millionen', 579: 'mindestens', 580: 'minuten', 581: 'mir', 582: 'mit', 583: 'mitarbeiter', 584: 'mitchell', 585: 'mitteilung', 586: 'mogherini', 587: 'moglichkeit', 588: 'monat', 589: 'monate', 590: 'monaten', 591: 'montag', 592: 'morgan', 593: 'morgen', 594: 'moskau', 595: 'musik', 596: 'muss', 597: 'musse', 598: 'mussen', 599: 'musste', 600: 'mussten', 601: 'mutter', 602: 'nach', 603: 'nachdem', 604: 'nachsten', 605: 'nachstes', 606: 'nacht', 607: 'nahe', 608: 'naher', 609: 'namen', 610: 'nationalen', 611: 'naturlich', 612: 'neben', 613: 'nehmen', 614: 'neu', 615: 'neue', 616: 'neuen', 617: 'neun', 618: 'new', 619: 'nicht', 620: 'nichts', 621: 'nie', 622: 'niemand', 623: 'noch', 624: 'norwegian', 625: 'november', 626: 'npd', 627: 'nun', 628: 'nur', 629: 'nussbaum', 630: 'ob', 631: 'oben', 632: 'obwohl', 633: 'oder', 634: 'of', 635: 'offentlichen', 636: 'oft', 637: 'ohne', 638: 'online', 639: 'operation', 640: 'opfer', 641: 'organisation', 642: 'organisatoren', 643: 'ort', 644: 'ortliche', 645: 'ortlichen', 646: 'ostukraine', 647: 'paar', 648: 'partei', 649: 'partner', 650: 'patienten', 651: 'perry', 652: 'piffl', 653: 'plane', 654: 'platz', 655: 'plotzlich', 656: 'politischen', 657: 'polizei', 658: 'polizisten', 659: 'position', 660: 'potzl', 661: 'prasident', 662: 'preis', 663: 'premier', 664: 'premierminister', 665: 'prestige', 666: 'pro', 667: 'problem', 668: 'probleme', 669: 'programm', 670: 'projekt', 671: 'proteste', 672: 'prozent', 673: 'putin', 674: 'qadri', 675: 'radfahrer', 676: 'radler', 677: 'rasse', 678: 'raum', 679: 'recht', 680: 'rechte', 681: 'regen', 682: 'regierung', 683: 'region', 684: 'regionen', 685: 'reihe', 686: 'rennen', 687: 'renzi', 688: 'residenz', 689: 'richter', 690: 'richtig', 691: 'richtige', 692: 'richtung', 693: 'rolle', 694: 'romischen', 695: 'ruckkehr', 696: 'rucktritt', 697: 'rund', 698: 'russische', 699: 'russischen', 700: 'russland', 701: 'sache', 702: 'sagen', 703: 'sagt', 704: 'sagte', 705: 'sagten', 706: 'sah', 707: 'salzhausen', 708: 'samstag', 709: 'schaffen', 710: 'scheint', 711: 'schlecht', 712: 'schlielich', 713: 'schlieung', 714: 'schmerz', 715: 'schnell', 716: 'schon', 717: 'schotten', 718: 'schrammel', 719: 'schrieb', 720: 'schule', 721: 'schulen', 722: 'schuler', 723: 'schutzenverein', 724: 'schwarze', 725: 'schwarzen', 726: 'schwer', 727: 'sechs', 728: 'sehen', 729: 'sehr', 730: 'sei', 731: 'seien', 732: 'sein', 733: 'seine', 734: 'seinem', 735: 'seinen', 736: 'seiner', 737: 'seines', 738: 'seit', 739: 'seite', 740: 'seiten', 741: 'selbst', 742: 'semester', 743: 'separatisten', 744: 'september', 745: 'service', 746: 'sharif', 747: 'sich', 748: 'sicher', 749: 'sicherheit', 750: 'sie', 751: 'sieben', 752: 'sieht', 753: 'sind', 754: 'so', 755: 'sogar', 756: 'sohn', 757: 'solche', 758: 'soldaten', 759: 'soll', 760: 'sollen', 761: 'sollte', 762: 'sollten', 763: 'sommer', 764: 'sondern', 765: 'sonntag', 766: 'sonst', 767: 'sorgen', 768: 'sorgte', 769: 'sowie', 770: 'sowohl', 771: 'spa', 772: 'spater', 773: 'spiel', 774: 'spielen', 775: 'spieler', 776: 'sport', 777: 'sporthalle', 778: 'sprach', 779: 'sprechen', 780: 'sprecher', 781: 'staat', 782: 'staaten', 783: 'staates', 784: 'staatliche', 785: 'staatlichen', 786: 'stadt', 787: 'stadten', 788: 'stand', 789: 'standen', 790: 'stark', 791: 'starke', 792: 'status', 793: 'stehen', 794: 'steht', 795: 'steigt', 796: 'stellen', 797: 'stellte', 798: 'sterben', 799: 'steuereinnahmen', 800: 'steuern', 801: 'stewart', 802: 'stiegen', 803: 'stimme', 804: 'stoff', 805: 'strae', 806: 'straen', 807: 'stuck', 808: 'studenten', 809: 'studie', 810: 'stunden', 811: 'tag', 812: 'tagen', 813: 'tages', 814: 'tat', 815: 'tatsachlich', 816: 'tausende', 817: 'team', 818: 'teil', 819: 'teilnehmer', 820: 'teilte', 821: 'teilten', 822: 'terroristen', 823: 'texas', 824: 'the', 825: 'thema', 826: 'thomas', 827: 'tochter', 828: 'tod', 829: 'tor', 830: 'torpedo', 831: 'tot', 832: 'tour', 833: 'treasure', 834: 'treffen', 835: 'trotz', 836: 'tun', 837: 'tweet', 838: 'twitch', 839: 'uber', 840: 'uberall', 841: 'uberhaupt', 842: 'uberleben', 843: 'uhr', 844: 'ukraine', 845: 'ukrainische', 846: 'um', 847: 'und', 848: 'unk', 849: 'uns', 850: 'unser', 851: 'unsere', 852: 'unseren', 853: 'unserer', 854: 'unter', 855: 'unternehmen', 856: 'unternehmenssteuern', 857: 'unterstutzung', 858: 'usa', 859: 'vater', 860: 'veranstalter', 861: 'verbindung', 862: 'verboten', 863: 'verdient', 864: 'verein', 865: 'verfugen', 866: 'verfugung', 867: 'vergangene', 868: 'vergangenen', 869: 'vergangenheit', 870: 'vergessen', 871: 'vergleich', 872: 'verhandlungen', 873: 'verkauf', 874: 'verkauft', 875: 'verlangt', 876: 'verlassen', 877: 'verletzt', 878: 'verlieren', 879: 'verloren', 880: 'vermutet', 881: 'vermutlich', 882: 'veroffentlicht', 883: 'verstand', 884: 'verstehen', 885: 'versuchen', 886: 'vertreter', 887: 'verwendet', 888: 'viel', 889: 'viele', 890: 'vielen', 891: 'vielleicht', 892: 'vier', 893: 'viertel', 894: 'virginia', 895: 'vom', 896: 'von', 897: 'vor', 898: 'voraussichtlich', 899: 'vorgehen', 900: 'vorgestellt', 901: 'vorsitzende', 902: 'vorsitzender', 903: 'wagen', 904: 'wahl', 905: 'wahlen', 906: 'wahlrecht', 907: 'wahrend', 908: 'war', 909: 'ware', 910: 'waren', 911: 'warnte', 912: 'warum', 913: 'was', 914: 'washington', 915: 'wasser', 916: 'weende', 917: 'weg', 918: 'wegen', 919: 'wei', 920: 'weie', 921: 'weien', 922: 'weier', 923: 'weil', 924: 'weise', 925: 'weit', 926: 'weiter', 927: 'weitere', 928: 'weiteren', 929: 'weiterhin', 930: 'welt', 931: 'wenig', 932: 'wenige', 933: 'weniger', 934: 'wenn', 935: 'wer', 936: 'werde', 937: 'werden', 938: 'wichtig', 939: 'wichtige', 940: 'wichtiger', 941: 'wie', 942: 'wieder', 943: 'wiederaufbau', 944: 'will', 945: 'wir', 946: 'wird', 947: 'wirklich', 948: 'wirtschaft', 949: 'wirtschaftlichen', 950: 'wissen', 951: 'wo', 952: 'woche', 953: 'wochen', 954: 'wochenende', 955: 'wohl', 956: 'wolfratshauser', 957: 'wollen', 958: 'wollte', 959: 'wollten', 960: 'worden', 961: 'wort', 962: 'wunschte', 963: 'wurde', 964: 'wurden', 965: 'wusste', 966: 'youtube', 967: 'zahlen', 968: 'zehn', 969: 'zeigen', 970: 'zeigt', 971: 'zeigte', 972: 'zeigten', 973: 'zeit', 974: 'zeitung', 975: 'ziehen', 976: 'ziel', 977: 'zu', 978: 'zudem', 979: 'zufolge', 980: 'zugang', 981: 'zukunft', 982: 'zum', 983: 'zur', 984: 'zuruck', 985: 'zuruckgekehrt', 986: 'zusammen', 987: 'zuvor', 988: 'zwar', 989: 'zwei', 990: 'zweiten', 991: 'zwischen'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2SKleAlZwdtA"
      },
      "source": [
        "### 3b)-Train - Test Split\n",
        "\n",
        "For validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kj_K6FfeAeYC",
        "outputId": "78aaae62-4d4f-4603-d1c1-f3126941473a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X, y = lines.eng, lines.ger #X being input, y being target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1735,), (434,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LZYt7v-fCOQt"
      },
      "source": [
        "**Save the train and test dataframes for reproducing the results later, as they are shuffled**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GvpxI3i7A3BT",
        "colab": {}
      },
      "source": [
        "X_train.to_pickle('X_train.pkl')\n",
        "X_test.to_pickle('X_test.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f99GUE2iA3E5",
        "colab": {}
      },
      "source": [
        "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
        "    ''' Generate a batch of data '''\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text.split()):\n",
        "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
        "                for t, word in enumerate(target_text.split()):\n",
        "                    if t<len(target_text.split())-1:\n",
        "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the START_ token\n",
        "                        # Offset by one timestep\n",
        "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G_MQxfbrCj_E"
      },
      "source": [
        "# 4)-Encoder - Decoder Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WAKup9U3A3II",
        "colab": {}
      },
      "source": [
        "latent_dim = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NVWXpjPeA3K4",
        "outputId": "04490d58-5313-421d-cc66-d6ce3a96ecfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y5NPVx5SAebD",
        "colab": {}
      },
      "source": [
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jmJpG9LHAegu",
        "outputId": "24a3cc6e-9eea-4a3f-d725-05a9f3640226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZYb2KO_TAepD",
        "colab": {}
      },
      "source": [
        "#from IPython.display import Image\n",
        "#Image(retina=True, filename='train.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dEJCmevEAesV",
        "colab": {}
      },
      "source": [
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "batch_size = 128\n",
        "epochs = 15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bYQOjYpjEmfJ",
        "outputId": "994427d4-d742-498a-ddec-5b68105aa2d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 50)     60550       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 50)     49600       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 50), (None,  20200       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 50), ( 20200       embedding_2[0][0]                \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 992)    50592       lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 201,142\n",
            "Trainable params: 201,142\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8_dLHRGWAemn",
        "outputId": "3e8e6f8d-77b3-492a-ffd8-636c4a1df5bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "source": [
        "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
        "                    steps_per_epoch = train_samples//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
        "                    validation_steps = val_samples//batch_size)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Epoch 1/15\n",
            "13/13 [==============================] - 9s 681ms/step - loss: 6.5216 - acc: 0.2631 - val_loss: 5.6383 - val_acc: 0.2989\n",
            "Epoch 2/15\n",
            "13/13 [==============================] - 4s 271ms/step - loss: 5.1901 - acc: 0.2954 - val_loss: 4.8398 - val_acc: 0.2927\n",
            "Epoch 3/15\n",
            "13/13 [==============================] - 4s 274ms/step - loss: 4.6318 - acc: 0.2959 - val_loss: 4.5492 - val_acc: 0.2949\n",
            "Epoch 4/15\n",
            "13/13 [==============================] - 4s 273ms/step - loss: 4.4850 - acc: 0.2973 - val_loss: 4.5317 - val_acc: 0.2922\n",
            "Epoch 5/15\n",
            "13/13 [==============================] - 4s 273ms/step - loss: 4.4619 - acc: 0.2962 - val_loss: 4.4609 - val_acc: 0.2989\n",
            "Epoch 6/15\n",
            "13/13 [==============================] - 4s 272ms/step - loss: 4.4652 - acc: 0.2936 - val_loss: 4.4981 - val_acc: 0.2927\n",
            "Epoch 7/15\n",
            "13/13 [==============================] - 4s 273ms/step - loss: 4.4485 - acc: 0.2948 - val_loss: 4.4708 - val_acc: 0.2949\n",
            "Epoch 8/15\n",
            "13/13 [==============================] - 4s 273ms/step - loss: 4.4298 - acc: 0.2964 - val_loss: 4.4944 - val_acc: 0.2922\n",
            "Epoch 9/15\n",
            "13/13 [==============================] - 4s 274ms/step - loss: 4.4138 - acc: 0.2970 - val_loss: 4.4315 - val_acc: 0.2989\n",
            "Epoch 10/15\n",
            "13/13 [==============================] - 4s 273ms/step - loss: 4.4211 - acc: 0.2941 - val_loss: 4.4606 - val_acc: 0.2927\n",
            "Epoch 11/15\n",
            "13/13 [==============================] - 4s 271ms/step - loss: 4.3998 - acc: 0.2961 - val_loss: 4.4354 - val_acc: 0.2949\n",
            "Epoch 12/15\n",
            "13/13 [==============================] - 4s 271ms/step - loss: 4.3901 - acc: 0.2955 - val_loss: 4.4622 - val_acc: 0.2922\n",
            "Epoch 13/15\n",
            "13/13 [==============================] - 4s 273ms/step - loss: 4.3729 - acc: 0.2964 - val_loss: 4.4032 - val_acc: 0.2989\n",
            "Epoch 14/15\n",
            "13/13 [==============================] - 4s 270ms/step - loss: 4.3749 - acc: 0.2948 - val_loss: 4.4345 - val_acc: 0.2927\n",
            "Epoch 15/15\n",
            "13/13 [==============================] - 4s 271ms/step - loss: 4.3526 - acc: 0.2975 - val_loss: 4.4100 - val_acc: 0.2949\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f589fa06048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S5p1pg6XAekB",
        "colab": {}
      },
      "source": [
        "# save model\n",
        "model.save_weights('translate.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bg0C_Xo0Aedy",
        "colab": {}
      },
      "source": [
        "model.load_weights('translate.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5MuBJTzQNTVn"
      },
      "source": [
        "# Inference Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DYLNM2LFNMAp",
        "colab": {}
      },
      "source": [
        "# Encode the input sequence to get the \"thought vectors\"\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yp5DWmhDNaSX"
      },
      "source": [
        "# Decode sample sequeces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qZnHAo2KNX1R",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    \n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index['START_']\n",
        "  \n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '_END' or\n",
        "           len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BmOR00fXNgFe"
      },
      "source": [
        "# Evaluation on Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vm_zRElrNd39",
        "colab": {}
      },
      "source": [
        "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
        "k=-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gUFD9t9ENhbG",
        "outputId": "5fca554b-3011-4c62-dce3-270d16a0e16d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', X_train[k:k+1].values[0])\n",
        "print('Actual German Translation:', y_train[k:k+1].values[0][6:-4])\n",
        "print('Predicted German Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input English sentence: the organization went unk with no issues and there were no unk unk unk unk unk unk with unk to the event\n",
            "Actual German Translation:  die organisation unk ohne probleme und es gab keine unk unk unk unk unk ein unk unk dieser unk \n",
            "Predicted German Translation:  unk unk unk unk unk unk unk unk unk unk unk unk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x5j_RvtnNheP",
        "outputId": "b78bec0f-9c4e-4c0d-b3df-26c161034484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', X_train[k:k+1].values[0])\n",
        "print('Actual German Translation:', y_train[k:k+1].values[0][6:-4])\n",
        "print('Predicted German Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input English sentence: it was not only the unk that unk unk unk in the unk at the park during unk first roman day\n",
            "Actual German Translation:  nicht nur die unk standen unk im unk beim ersten unk unk im unk \n",
            "Predicted German Translation:  unk unk unk unk unk unk unk unk unk unk unk unk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t7KzAzWTNhhh",
        "outputId": "7f681bf1-f090-4f4d-c222-831b8f41cd26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', X_train[k:k+1].values[0])\n",
        "print('Actual German Translation:', y_train[k:k+1].values[0][6:-4])\n",
        "print('Predicted German Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input English sentence: the unk and unk by unk unk song unk unk political unk and unk is really more a statement of unk than it is a unk\n",
            "Actual German Translation:  es ist unk mehr unk unk als unk was unk unk unk unk unk unk unk in unk und unk auf seiten zu unk gebracht hat \n",
            "Predicted German Translation:  unk unk unk unk unk unk unk unk unk unk unk unk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2tpV8t7HOc8R"
      },
      "source": [
        "# Evaluation on Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ew_6eBkXNhkm",
        "colab": {}
      },
      "source": [
        "val_gen = generate_batch(X_test, y_test, batch_size = 1)\n",
        "k=-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JAc4u_sbOgV8",
        "outputId": "af4bac25-ebff-4f4b-a4a6-60b8d9c145f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(val_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', X_test[k:k+1].values[0])\n",
        "print('Actual German Translation:', y_test[k:k+1].values[0][6:-4])\n",
        "print('Predicted German Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input English sentence: and he has been unk honest throughout from unk as a unk school unk to unk and unk he has left unk of his human unk unk\n",
            "Actual German Translation:  und er ist dabei unk ehrlich von der unk als unk bis zum unk und unk lasst er keine unk unk aus \n",
            "Predicted German Translation:  unk unk unk unk unk unk unk unk unk unk unk unk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6qjx9jmtOlBn",
        "outputId": "3543c53c-901e-47bf-fe38-839742ec1b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(val_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', X_test[k:k+1].values[0])\n",
        "print('Actual German Translation:', y_test[k:k+1].values[0][6:-4])\n",
        "print('Predicted German Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input English sentence: ebola the epidemic is unk unk on west unk economy\n",
            "Actual German Translation:  unk die unk unk unk wirtschaft \n",
            "Predicted German Translation:  unk unk unk unk unk unk unk unk unk unk unk unk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mt_UxJJhOt8l",
        "outputId": "c5d830c6-582b-4b52-e41f-13f9fdbc8bc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(val_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', X_test[k:k+1].values[0])\n",
        "print('Actual German Translation:', y_test[k:k+1].values[0][6:-4])\n",
        "print('Predicted German Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input English sentence: unk piffl the deputy chairman of castle friends said himself that he was very unk about the call\n",
            "Actual German Translation:  unk piffl unk vorsitzender der burgfreunde hat sich nach eigenen angaben uber den unk sehr unk \n",
            "Predicted German Translation:  unk unk unk unk unk unk unk unk unk unk unk unk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzPN7jzAYFHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}