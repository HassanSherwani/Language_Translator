{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJxJLfT6m4zo"
   },
   "source": [
    "# Machine Translation\n",
    "\n",
    "German-English using sparse categorical_accuracy as evaluation matrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YvFSu3KOnCQI"
   },
   "source": [
    "# 1)- Importing key modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ydSdCo4JzRta"
   },
   "outputs": [],
   "source": [
    "#support both Python 2 and Python 3 with minimal overhead.\n",
    "from __future__ import absolute_import, division, print_function\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>body {\n",
       "    margin: 0;\n",
       "    font-family: Helvetica;\n",
       "}\n",
       "table.dataframe {\n",
       "    border-collapse: collapse;\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe tr {\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe td, table.dataframe th {\n",
       "    margin: 0;\n",
       "    border: 1px solid white;\n",
       "    padding-left: 0.25em;\n",
       "    padding-right: 0.25em;\n",
       "}\n",
       "table.dataframe th:not(:empty) {\n",
       "    background-color: #fec;\n",
       "    text-align: left;\n",
       "    font-weight: normal;\n",
       "}\n",
       "table.dataframe tr:nth-child(2) th:empty {\n",
       "    border-left: none;\n",
       "    border-right: 1px dashed #888;\n",
       "}\n",
       "table.dataframe td {\n",
       "    border: 2px solid #ccf;\n",
       "    background-color: #f4f4ff;\n",
       "}\n",
       "h3 {\n",
       "    color: white;\n",
       "    background-color: black;\n",
       "    padding: 0.5em;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's life without style :). So, let's add style to our dataframes\n",
    "from IPython.core.display import HTML\n",
    "css = open('style-table.css').read() + open('style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXdjBXc-zNXe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import string \n",
    "import re \n",
    "import unicodedata\n",
    "from numpy import array, argmax, random, take \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "% matplotlib inline \n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oAKOREWdCLPr"
   },
   "outputs": [],
   "source": [
    "# for seq2seq model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import optimizers \n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, CuDNNLSTM, Flatten, TimeDistributed, Dropout, LSTMCell, RNN\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model \n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext version_information\n",
    "%version_information pandas,re,sklearn, matplotlib,keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XN_b91atnHye"
   },
   "source": [
    "# 2)- Reading Dataset & quick preprocessing\n",
    "\n",
    "We will not be using extension step by step as there is a separate file for that. In fact , data is pretty much clean. It is only a double check and to follow all proper steps of preprocessing and data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYR22vaiC5UG"
   },
   "outputs": [],
   "source": [
    "path_to_file = \"random_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eziqj57OCQOj"
   },
   "outputs": [],
   "source": [
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "        self.vocab = sorted(self.vocab)\n",
    "        self.word2idx[\"<pad>\"] = 0\n",
    "        self.idx2word[0] = \"<pad>\"\n",
    "        for i,word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = i + 1\n",
    "            self.idx2word[i+1] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pygpKzHnCQRZ"
   },
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    w = \"<start> \" + w + \" <end>\"\n",
    "    return w\n",
    "\n",
    "def max_length(t):\n",
    "    return max(len(i) for i in t)\n",
    "\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = open(path, encoding=\"UTF-8\").read().strip().split(\"\\n\")\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split(\"\\t\")] for l in lines[:num_examples]]\n",
    "    return word_pairs\n",
    "\n",
    "def load_dataset(path, num_examples):\n",
    "    pairs = create_dataset(path, num_examples)\n",
    "    out_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "    in_lang = LanguageIndex(en for en, sp in pairs)\n",
    "    input_data = [[in_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
    "    output_data = [[out_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
    "\n",
    "    max_length_in, max_length_out = max_length(input_data), max_length(output_data)\n",
    "    input_data = tf.keras.preprocessing.sequence.pad_sequences(input_data, maxlen=max_length_in, padding=\"post\")\n",
    "    output_data = tf.keras.preprocessing.sequence.pad_sequences(output_data, maxlen=max_length_out, padding=\"post\")\n",
    "    return input_data, output_data, in_lang, out_lang, max_length_in, max_length_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v88OB32ACQUI"
   },
   "outputs": [],
   "source": [
    "#num_examples = 118000 # Full example set.\n",
    "num_examples = 30000 # Partial set for faster training\n",
    "input_data, teacher_data, input_lang, target_lang, len_input, len_target = load_dataset(path_to_file, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dqTixoujCQXR"
   },
   "outputs": [],
   "source": [
    "target_data = [[teacher_data[n][i+1] for i in range(len(teacher_data[n])-1)] for n in range(len(teacher_data))]\n",
    "target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, maxlen=len_target, padding=\"post\")\n",
    "target_data = target_data.reshape((target_data.shape[0], target_data.shape[1], 1))\n",
    "\n",
    "# Shuffle all of the data in unison. This training set has the longest (e.g. most complicated) data at the end,\n",
    "# so a simple Keras validation split will be problematic if not shuffled.\n",
    "p = np.random.permutation(len(input_data))\n",
    "input_data = input_data[p]\n",
    "teacher_data = teacher_data[p]\n",
    "target_data = target_data[p]\n",
    "\n",
    "BUFFER_SIZE = len(input_data)\n",
    "BATCH_SIZE = 64\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_in_size = len(input_lang.word2idx)\n",
    "vocab_out_size = len(target_lang.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_I0SUICZDtoi"
   },
   "source": [
    "# 3)-Creating the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "ISPFDcU3CQdC",
    "outputId": "d5a93bbe-405c-4f8b-a7ea-885bfeaa51fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Create the Encoder layers first.\n",
    "encoder_inputs = Input(shape=(len_input,))\n",
    "encoder_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
    "encoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Now create the Decoder layers.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\n",
    "decoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n",
    "decoder_lstm_out, _, _ = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n",
    "# Two dense layers added to this model to improve inference capabilities.\n",
    "decoder_d1 = Dense(units, activation=\"relu\")\n",
    "decoder_d2 = Dense(vocab_out_size, activation=\"softmax\")\n",
    "# Drop-out is added in the dense layers to help mitigate overfitting in this part of the model. Astute developers\n",
    "# may want to add the same mechanism inside the LSTMs.\n",
    "decoder_out = decoder_d2(Dropout(rate=.4)(decoder_d1(Dropout(rate=.4)(decoder_lstm_out))))\n",
    "\n",
    "# Finally, create a training model which combines the encoder and the decoder.\n",
    "# Note that this model has three inputs:\n",
    "#  encoder_inputs=[batch,encoded_words] from input language (English)\n",
    "#  decoder_inputs=[batch,encoded_words] from output language (Spanish). This is the \"teacher tensor\".\n",
    "#  decoder_out=[batch,encoded_words] from output language (Spanish). This is the \"target tensor\".\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_out)\n",
    "# We'll use sparse_categorical_crossentropy so we don't have to expand decoder_out into a massive one-hot array.\n",
    "#  Adam is used because it's, well, the best.\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "UOQymUzOEYWp",
    "outputId": "8f4170a4-3458-4bda-ffb7-edec28adbfef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 73)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 73, 256)      1851648     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    2377216     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)          [(None, 73, 1024), ( 5251072     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, None, 1024), 5251072     embedding_1[0][0]                \n",
      "                                                                 cu_dnnlstm[0][1]                 \n",
      "                                                                 cu_dnnlstm[0][2]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 1024)   0           cu_dnnlstm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 1024)   1049600     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 1024)   0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 9286)   9518150     dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 25,298,758\n",
      "Trainable params: 25,298,758\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "colab_type": "code",
    "id": "plUWeITuCQgK",
    "outputId": "e6a42e07-6680-4972-cbc5-98dba934d861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5205 samples, validate on 1302 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/15\n",
      "5205/5205 [==============================] - 44s 8ms/sample - loss: 2.1725 - sparse_categorical_accuracy: 0.7478 - val_loss: 1.7911 - val_sparse_categorical_accuracy: 0.7638\n",
      "Epoch 2/15\n",
      "5205/5205 [==============================] - 41s 8ms/sample - loss: 1.7306 - sparse_categorical_accuracy: 0.7656 - val_loss: 1.7204 - val_sparse_categorical_accuracy: 0.7646\n",
      "Epoch 3/15\n",
      "5205/5205 [==============================] - 41s 8ms/sample - loss: 1.6512 - sparse_categorical_accuracy: 0.7677 - val_loss: 1.6769 - val_sparse_categorical_accuracy: 0.7656\n",
      "Epoch 4/15\n",
      "5205/5205 [==============================] - 41s 8ms/sample - loss: 1.5919 - sparse_categorical_accuracy: 0.7689 - val_loss: 1.6263 - val_sparse_categorical_accuracy: 0.7669\n",
      "Epoch 5/15\n",
      "5205/5205 [==============================] - 41s 8ms/sample - loss: 1.5338 - sparse_categorical_accuracy: 0.7708 - val_loss: 1.5741 - val_sparse_categorical_accuracy: 0.7688\n",
      "Epoch 6/15\n",
      "5205/5205 [==============================] - 41s 8ms/sample - loss: 1.4756 - sparse_categorical_accuracy: 0.7728 - val_loss: 1.5272 - val_sparse_categorical_accuracy: 0.7705\n",
      "Epoch 7/15\n",
      "5205/5205 [==============================] - 41s 8ms/sample - loss: 1.4919 - sparse_categorical_accuracy: 0.7722 - val_loss: 1.5355 - val_sparse_categorical_accuracy: 0.7699\n",
      "Epoch 8/15\n",
      "5205/5205 [==============================] - 41s 8ms/sample - loss: 1.4068 - sparse_categorical_accuracy: 0.7752 - val_loss: 1.4477 - val_sparse_categorical_accuracy: 0.7736\n",
      "Epoch 9/15\n",
      "5205/5205 [==============================] - 41s 8ms/sample - loss: 1.3293 - sparse_categorical_accuracy: 0.7788 - val_loss: 1.3822 - val_sparse_categorical_accuracy: 0.7772\n",
      "Epoch 10/15\n",
      "5205/5205 [==============================] - 41s 8ms/sample - loss: 1.2640 - sparse_categorical_accuracy: 0.7820 - val_loss: 1.3186 - val_sparse_categorical_accuracy: 0.7799\n",
      "Epoch 11/15\n",
      "5205/5205 [==============================] - 41s 8ms/sample - loss: 1.1978 - sparse_categorical_accuracy: 0.7853 - val_loss: 1.2521 - val_sparse_categorical_accuracy: 0.7830\n",
      "Epoch 12/15\n",
      "5205/5205 [==============================] - 41s 8ms/sample - loss: 1.1307 - sparse_categorical_accuracy: 0.7896 - val_loss: 1.1850 - val_sparse_categorical_accuracy: 0.7876\n",
      "Epoch 13/15\n",
      "5205/5205 [==============================] - 41s 8ms/sample - loss: 1.0639 - sparse_categorical_accuracy: 0.7944 - val_loss: 1.1186 - val_sparse_categorical_accuracy: 0.7933\n",
      "Epoch 14/15\n",
      "5205/5205 [==============================] - 41s 8ms/sample - loss: 0.9956 - sparse_categorical_accuracy: 0.8014 - val_loss: 1.0449 - val_sparse_categorical_accuracy: 0.8031\n",
      "Epoch 15/15\n",
      "5205/5205 [==============================] - 41s 8ms/sample - loss: 0.9229 - sparse_categorical_accuracy: 0.8099 - val_loss: 0.9715 - val_sparse_categorical_accuracy: 0.8140\n"
     ]
    }
   ],
   "source": [
    "# Note, we use 20% of our data for validation.\n",
    "epochs = 15\n",
    "history = model.fit([input_data, teacher_data], target_data,\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 epochs=epochs,\n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OTG8zJyNDxqK"
   },
   "source": [
    "# 4)-Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C693eOhxCQjC"
   },
   "outputs": [],
   "source": [
    "# Create the encoder model from the tensors we previously declared.\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Generate a new set of tensors for our new inference decoder. Note that we are using new tensors, \n",
    "# this does not preclude using the same underlying layers that we trained on. (e.g. weights/biases).\n",
    "inf_decoder_inputs = Input(shape=(None,), name=\"inf_decoder_inputs\")\n",
    "# We'll need to force feed the two state variables into the decoder each step.\n",
    "state_input_h = Input(shape=(units,), name=\"state_input_h\")\n",
    "state_input_c = Input(shape=(units,), name=\"state_input_c\")\n",
    "decoder_res, decoder_h, decoder_c = decoder_lstm(\n",
    "    decoder_emb(inf_decoder_inputs), \n",
    "    initial_state=[state_input_h, state_input_c])\n",
    "inf_decoder_out = decoder_d2(decoder_d1(decoder_res))\n",
    "inf_model = Model(inputs=[inf_decoder_inputs, state_input_h, state_input_c], \n",
    "                  outputs=[inf_decoder_out, decoder_h, decoder_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-vAVR-CbCQl7"
   },
   "outputs": [],
   "source": [
    "# Converts the given sentence (just a string) into a vector of word IDs\n",
    "# using the language specified. This can be used for either the input (English)\n",
    "# or target (German) languages.\n",
    "# Output is 1-D: [timesteps/words]\n",
    "def sentence_to_vector(sentence, lang):\n",
    "    pre = preprocess_sentence(sentence)\n",
    "    vec = np.zeros(len_input)\n",
    "    sentence_list = [lang.word2idx[s] for s in pre.split(' ')]\n",
    "    for i,w in enumerate(sentence_list):\n",
    "        vec[i] = w\n",
    "    return vec\n",
    "\n",
    "# Given an input string, an encoder model (infenc_model) and a decoder model (infmodel),\n",
    "# return a translated string.\n",
    "def translate(input_sentence, infenc_model, infmodel, attention=False):\n",
    "    sv = sentence_to_vector(input_sentence, input_lang)\n",
    "    # Reshape so we can use the encoder model. New shape=[samples,sequence length]\n",
    "    sv = sv.reshape(1,len(sv))\n",
    "    [emb_out, sh, sc] = infenc_model.predict(x=sv)\n",
    "    \n",
    "    i = 0\n",
    "    start_vec = target_lang.word2idx[\"<start>\"]\n",
    "    stop_vec = target_lang.word2idx[\"<end>\"]\n",
    "    # We will continuously feed cur_vec as an input into the decoder to produce the next word,\n",
    "    # which will be assigned to cur_vec. Start it with \"<start>\".\n",
    "    cur_vec = np.zeros((1,1))\n",
    "    cur_vec[0,0] = start_vec\n",
    "    cur_word = \"<start>\"\n",
    "    output_sentence = \"\"\n",
    "    # Start doing the feeding. Terminate when the model predicts an \"<end>\" or we reach the end\n",
    "    # of the max target language sentence length.\n",
    "    while cur_word != \"<end>\" and i < (len_target-1):\n",
    "        i += 1\n",
    "        if cur_word != \"<start>\":\n",
    "            output_sentence = output_sentence + \" \" + cur_word\n",
    "        x_in = [cur_vec, sh, sc]\n",
    "        # This will allow us to accomodate attention models, which we will talk about later.\n",
    "        if attention:\n",
    "            x_in += [emb_out]\n",
    "        [nvec, sh, sc] = infmodel.predict(x=x_in)\n",
    "        # The output of the model is a massive softmax vector with one spot for every possible word. Convert\n",
    "        # it to a word ID using argmax().\n",
    "        cur_vec[0,0] = np.argmax(nvec[0,0])\n",
    "        cur_word = target_lang.idx2word[np.argmax(nvec[0,0])]\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BkP3J2JZCQo_"
   },
   "outputs": [],
   "source": [
    "# Let's test out the model! Feel free to modify as you see fit. Note that only words\n",
    "# that we've trained the model on will be available, otherwise you'll get an error.\n",
    "#print(translate(\"india is also reportedly hoping for a deal on defence collaboration between the two nations\", encoder_model, inf_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-FtsBpmQnj_A"
   },
   "source": [
    "# 5)-Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "UGgBO8ginjFM",
    "outputId": "6761a3f7-5290-421a-b770-9d97ea62b0c5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VNX5x/HPkx3CFrKwhZCw74iE\nTRQFRXFFq1Zwq0tLrftWq62trd1sa2v9VbqIu6KoaBWtuLQiiLIFZF9DQhYCZCOBJGR/fn/cAYeQ\nZYBJJpl53q9XXjNz59yZZ9B85+Tcc88VVcUYY0xgCPJ1AcYYY1qOhb4xxgQQC31jjAkgFvrGGBNA\nLPSNMSaAWOgbY0wAsdA3xpgAYqFvjDEBxELfGGMCSIivC6grJiZGExMTfV2GMca0KWvWrMlX1dim\n2rW60E9MTCQlJcXXZRhjTJsiIhmetLPhHWOMCSAW+sYYE0As9I0xJoBY6BtjTACx0DfGmABioW+M\nMQHEQt8YYwJIq5unb4wxAWnjAud2+JUg0mxvYz19Y4zxtYM58OH9kPICNPN1yy30jTHGl1Thg3ug\nphJmPANBzRvLNrxjjDG+tO512PkpTP8DdO3b7G9nPX1jjPGVgznw8SOQcAaMm90ib2mhb4wxvtDC\nwzpH2PCOMcb4wvo3vh3Wie7XYm9rPX1jjGlpB3Ng0cMtOqxzhIW+Mca0JFX44N4WH9Y5wqN3E5Hp\nIrJdRFJF5OF6nk8QkcUi8o2IbBCRi1zbo13bS0TkGW8Xb4wxbc76+bDzEzjvsRYd1jmiydAXkWBg\nDnAhMBSYJSJD6zR7FHhLVUcDM4G/u7aXAz8HHvRaxcYY01Yd3Asf/wQSJsK4H/qkBE96+uOAVFVN\nU9VKYD4wo04bBTq57ncGcgBUtVRVl+GEvzHGBK4js3WqK2HGnBYf1jnCk9k7vYAst8fZwPg6bX4J\nfCoidwGRwHleqc4YY/zFkWGdC35f77DOqvRCVJXxfaObtQxvfdXMAl5S1XjgIuBVEfH4tUVktoik\niEhKXl6el0oyxphWwn1YZ/xtxz2dnl/K7FdTeGzhZmpqfb/2zh6gt9vjeNc2d7cCbwGo6nIgAojx\ntAhVfVZVk1U1OTY21tPdjDGm9VOFD++F6op6h3UOlFZyy0urEeBfN4whOKj5VtgEz0J/NTBARJJE\nJAznQO3COm0ygXMBRGQITuhbl90YYza8CTs+hnN/cdywTmV1LT98bQ17Dhzm2RuT6RMd2ezlNDmm\nr6rVInIn8AkQDLygqptF5HEgRVUXAg8Ac0XkPpyDujepOuuDishunIO8YSJyOXC+qm5pno9jjDGt\nyMG9sOgh6D3huGEdVeXhdzewKr2Qp2eextjEri1SkkfLMKjqR8BHdbb9wu3+FmBSA/smnkJ9xhjT\nNh03rBN8zNPPfJ7Ku2v3cN95A5lxWq8WK8vOyDXGmObgPqwT0/+Ypxauz+HPn+3gitG9uPvc/g28\nQPOw0DfGGG87tK/BYZ01GYU8+PZ6xiZG8cSVI5BmvDRifSz0jTHGm46srVPPsE5mQRmzX1lDj84R\n/OuGZMJDght5oeZhoW+MMd604S3YsQim/vyYYZ3iw1Xc/NIqqmuVF28aS9fIMJ+UZ6FvjDHecnRY\nZzxM+NHRzVU1tdw+bw2ZhWX864Yx9I3t4LMS7SIqxhjjDarw4X1QXX7MsI6q8vP3NvFVagFPXj2K\nCc28zEJTrKdvjDHesPFt2P4RTH0UYgYc3fzs0jTmr87izin9uWpMvA8LdFjoG2PMqTq0Dz76McSP\ngwm3H9388aa9PPHxNi4Z2YP7pw30YYHfstA3xphT4T6sc/nfjw7rrM8q4t4313Fa7y48efUogpp5\nTR1PWegbY8ypqGdYZ0/RYb7/SgoxHcKZe2MyEaEtPzWzIXYg1xhjTtah/a5hnbFHh3UOlVdxy4ur\nKa+q4fXvjyemQ7iPizyWhb4xxpyMI8M6VYdhhjOsU11Ty52vf0NqXgkv3zyOAd06+rrK49jwjjHG\nnIyNC2D7f5xhndiBqCq/+mALS3bk8ZvLh3PmAI8vKdKiLPSNMeZEHdoPi1zDOhPvAODFr3bz6ooM\nfji5L7PGJfi4wIZZ6BtjzIlQhf/cD5VlR4d1/rtlP7/+zxYuGNaNn0wf7OsKG2Whb4wxJ2LDm7Dt\nQ5j6M4gdyKY9xdw9/xtG9OrMX68Z3WqmZjbEQt8YYzyVvxM+vN+5wPnEO9lXXM6tL6+mS7tQnrsx\nmXZhrWdqZkNs9o4xxnii6jC8fROEhMOVz1Napdz68mpKK2p4+7aJxHWK8HWFHrHQN8YYT3z8MOzf\nBNctoKZjT+55NYWtew/y/E1jGdKjk6+r85gN7xhjTFM2LoA1L8Gke2HANH730Vb+uzWXX142jCmD\n4nxd3Qmx0DfGmMYU7IIP7nHWyJ/6KM8u3cXzy9K5eVIiN05M9HV1J8yj0BeR6SKyXURSReThep5P\nEJHFIvKNiGwQkYvcnnvEtd92EbnAm8UbY0yzqiqHt74HwaFw1Qv87Yvd/O6jbVw8ogePXjzU19Wd\nlCbH9EUkGJgDTAOygdUislBVt7g1exR4S1X/ISJDgY+ARNf9mcAwoCfwXxEZqKo13v4gxhjjdZ/8\nFPZvRGfN58kVJcxZvIvvjO7FH68aSXArn5rZEE96+uOAVFVNU9VKYD4wo04bBY4cyegM5LjuzwDm\nq2qFqqYDqa7XM8aY1m3Tu5DyPDrxLn6zsw9zFu9i1rjePHn1KEKC2+7IuCezd3oBWW6Ps4Hxddr8\nEvhURO4CIoHz3PZdUWffXidVqTHGtJSCXbDwbrTXWB4ru5JXVqZz0xmJPHbpUETaZg//CG99Xc0C\nXlLVeOAi4FUR8fi1RWS2iKSISEpeXp6XSjLGmJNQVQ5v34QGBfO79g/xysocbju7n18EPngW+nuA\n3m6P413b3N0KvAWgqsuBCCDGw31R1WdVNVlVk2NjYz2v3hhjvO3TR2HfBuZGP8TcjVXce94AfjJ9\nkF8EPngW+quBASKSJCJhOAdmF9ZpkwmcCyAiQ3BCP8/VbqaIhItIEjAAWOWt4o0xxqs2vwer5/JZ\n56v43a5EfjJ9MPeeN9BvAh88GNNX1WoRuRP4BAgGXlDVzSLyOJCiqguBB4C5InIfzkHdm1RVgc0i\n8hawBagG7rCZO8aYVqkwDV14J2lhg7l9/2U8dulQbp6U5OuqvE6cbG49kpOTNSUlxddlGGMCSXUF\nNc9No3z/Li4o/y23Xz6Va8e33jXx6yMia1Q1ual2tvaOMSbgVS76GWH71nNf1f3cf/V5fOf0eF+X\n1Gws9I0xAa103b+JXDOXF2su5LJrfsAlI3v6uqRmZaFvjAlYRXt2EPL+HWyo7Uf8d//ItBH+Hfhg\nC64ZYwJUbtFB9r1wLbW1yuHLn2PaiLY1hn+yLPSNMQFnb/Fhls65ncE1O8k550nGn366r0tqMRb6\nxpiAklVYxt/m/JWrqj5g/5CbGDzlOl+X1KJsTN8YEzDS8kp4YO4HvFz5N8piRtDtyj/6uqQWZ6Fv\njAkIO/cf4sa5y5hb/SSRoUEEX/uqc73bAGOhb4zxe5tzirnh+VU8oPMYTipc8Qp09b+zbT1hY/rG\nGL+2LquIWc+u4DxJ4braD2DcbBha95IggcN6+sYYv7V6dyE3v7iaIe2K+L3+E2JHwfm/8XVZPmU9\nfWOM36mtVV5bkcGNz6+iR8dg5kX9i2Bq4eqXAnIc35319I0xfiU9v5SH39nAyvRCzuwfw7Pd3yMs\nZY0T+F37+ro8n7PQN8b4heqaWp5fls5fPttBWEgQf7xyJFd32oS88XcY+30YdoWvS2wVLPSNMW3e\n1r0H+ck7G9iQXcz5Q7vx68uH061kK7z6I+g+As7/ra9LbDUs9I0xbVZFdQ1zPk/l71/sokv7UOZc\nezoXjeiOfPMa/OcBiIyFq1+G0Ahfl9pqWOgbY9qktZkH+MmCDezMLeE7o3vx80uGEhWu8OG9sOYl\n6HsOXPkCREb7uNLWxULfGNOmlFVW8+dPd/DCV+n06BTBizePZcqgOCjOhtdvhD1r4Mz7YOrPISjY\n1+W2Ohb6xpg246vUfB5+dwNZhYe5YUIfHpo+iI4RoZC2BBbcAtUVcM1rMORSX5faalnoG2NaveLD\nVfz+o63MX51FUkwkb86ewPi+0aAKXz0N//0lRA9wAj92oK/LbdUs9I0xrdpnW/bz6HsbyTtUwQ/P\n7st95w0kIjQYKg7B+3fAlvedZRVmzIHwjr4ut9XzKPRFZDrwNBAMPKeqT9R5/ilgiutheyBOVbu4\nnvsDcLHruV+r6pveKNwY49/ySyr45cLNfLhhL4O7d2TujcmMjO/iPJm3A968Hgp2wrRfwxl3gYhv\nC24jmgx9EQkG5gDTgGxgtYgsVNUtR9qo6n1u7e8CRrvuXwycDpwGhANfiMgiVT3o1U9hjPEbqsr7\n63L41QebKa2o4YFpA7ntnH6EBrtWjdmyEN673VlO4cb3IWmybwtuYzzp6Y8DUlU1DUBE5gMzgC0N\ntJ8FPOa6PxRYqqrVQLWIbACmA2+dUtXGGL+UU3SYR9/bxOfbchmd0IU/XjmSAd1cQzY11fD5484Y\nfq8x8N1XoXMv3xbcBnkS+r2ALLfH2cD4+hqKSB8gCfjctWk98JiI/Bln2GcKDX9ZGGMCVG2t8vqq\nTJ5YtI2aWuUXlwzle2ckEhzkGrIpzYcFN0P6UhhzM1z4h4BfOO1keftA7kxggarWAKjqpyIyFvga\nyAOWAzV1dxKR2cBsgISEwLgivTHGsX3fIX7+/iZWpRcyqX80v79iJAnR7b9tsGcNvHkjlOY5B2tH\nX++7Yv2AJ6G/B+jt9jjeta0+M4E73Deo6m+B3wKIyOvAjro7qeqzwLMAycnJ6kFNxpg2rvhwFX/9\n7w5eWZ5Bx4gQ/nDlCL6b3BtxPyC75iX46MfQoTvc+gn0HO2zev2FJ6G/GhggIkk4YT8TuLZuIxEZ\nDETh9OaPbAsGuqhqgYiMBEYCn3qjcGNM21Rbq7yzNps/fLyNgtJKrhufwAPTBhEVGfZto6py+OhB\n+OZV6DsFrnoB2nf1XdF+pMnQV9VqEbkT+ARnyuYLqrpZRB4HUlR1oavpTGC+qrr31EOBL13f3AeB\n610HdY0xAWhjdjG/WLiJbzKLOD2hCy/dPI7hvTof26goE966EXK+gbMehCk/teUUvEiOzWjfS05O\n1pSUFF+XYYzxosLSSv70yXbmr84kOjKcRy4czBWjexEUVGdu/a7FznIKtdVwxT9h8MX1v6A5jois\nUdXkptrZGbnGmGZT45qV8+Qn2ympqOaWSUncc94AOkWEHttQFZY9BZ//GmIGOcspxPT3TdF+zkLf\nGNMs1mQU8vP3NrNl70Em9o3mVzOGMbBbPcskHNoHH94H2z+CYd+By/4G4R1avuAAYaFvjPGq3EPl\nPLFoG++u3UOPzhE8c+1oLh7R49hZOQC1tbDmRfjvr6C6HC74PUz4kS2n0Mws9I0xXlFVU8vLX+/m\nr//dSWV1LXdM6ccdU/rTPqyemMndCh/cA1krIfEsuPRpiO7X8kUHIAt9Y8wp+zo1n8cWbmZnbgnn\nDIrlsUuHkRQTeXzDqnJY+idnKYXwjnD5P2DULOvdtyALfWPMScspOsxv/7OV/2zcS++u7XjuxmTO\nHRJ3/FAOOBc6+fBeKEyDkTPhgt9CZEzLFx3gLPSNMSesorqG575M55nPU1GU+6cNZPbkvs4693WV\nFsCnj8L61yEqCW54D/pNOb6daREW+saYE7J4Wy6/+mAzuwvKuHB4d3528RDio9of31AVNrwJn/wU\nyovhrAdg8o8htF3LF22OstA3xjRJVfl6VwH/+GIXy1Lz6Rsbyau3juOsAbH171CwC/5zP6R9AfFj\nnQO13Ya1aM2mfhb6xpgGVdfU8vHmffxrSRob9xQT0yGcRy8ewo0TEwkLCTp+h5oq+Pr/YMkfITgM\nLnoSkm+FoHraGp+w0DfGHKe8qoa3U7KY+2U6mYVlJMVE8vvvjOCK0b3qH7cHyFrlTMPM3QJDLoUL\n/widerZs4aZJFvrGmKOKyip5ZXkGL3+9m4LSSk7r3YWfXjSEaUO7fXtBk7rKi+F/j8Pq552Qn/kG\nDL6oZQs3HrPQN8aQfaCM55el8+bqLMoqa5gyKJbbzu7HuKSu9U+/BOdA7dYPYNFDzlIK42+DqT9z\n5t+bVstC35gAtnXvQZ5dmsbC9TkIcNlpPZk9uS+Du3dqfMfibOfiJts/gm4jYOY857q1ptWz0Dcm\nwKgqK9IK+eeSXSzZkUf7sGBuOiORW85MoleXJqZT1tbAqrnOapi1NTDt1zDhdgi2KGkr7L+UMQGi\nplb5dPM+/rlkF+uzi4mODOPB8wdyw4REOrcPbXznihLYMB9W/gvyd0D/8+DiP0NUYovUbrzHQt8Y\nP1deVcM7a7OZuzSN3QVl9Iluz28uH85VY+IbnolzRGE6rH4O1r4KFcXONWqvfhmGzrD1ctooC31j\n/FRxWRWvrczgxa92k19Swcj4zvz9utO5YFj3hmfigHOANn0prPwnbF/kXKpw6AznQG38WAv7Ns5C\n3xg/k1lQxotfp/PW6ixKK2uYPDCW287uy8S+0Q3PxAGoLHOWTVj5L8jbCu1jYPKDkHyLzbf3Ixb6\nxvgBVWVleiEvLEvns637CRbhkpE9mD25H0N7NjETpyjTGcJZ8zKUF0H3ETDj7zD8SgiNaJkPYFqM\nhb4xbVhFdQ0frt/LC1+lsznnIFHtQ7njnP7cMLEP3To1EtiqkPE1rPwHbPsPIDDkEhj/I0iYYEM4\nfsyj0BeR6cDTQDDwnKo+Uef5p4Aja6W2B+JUtYvruT8CFwNBwGfAPaqq3infmMBUUFLBvJWZvLoi\ng7xDFQyI68DvvzOCy0/rRbuwRg7OVh2GjQucIZz9G6FdFEy6x1kfp0vvlvsAxmeaDH0RCQbmANOA\nbGC1iCxU1S1H2qjqfW7t7wJGu+6fAUwCRrqeXgacDXzhpfqNCSjb9x3ihWXp/HvdHiqrazlnUCy3\nTErirAExjY/XF++BlOch5UU4XAhxQ+HS/4MRV0NYPcsiG7/lSU9/HJCqqmkAIjIfmAFsaaD9LOAx\n130FIoAwQIBQYP+pFGxMoKmtVZbsyOP5ZeksS80nIjSIq8fEc/OkRPrHNbLkgapzDdqV/4QtC0Fr\nYfDFMP6HznVpbQgnIHkS+r2ALLfH2cD4+hqKSB8gCfgcQFWXi8hiYC9O6D+jqltPqWJjAkRZZTXv\nrMnmxa92k5ZfSvdOETw0fRCzxiYQFRnW8I6Hi2Dzu86B2b3rILwzTPgRjPuBnUxlvH4gdyawQFVr\nAESkPzAEiHc9/5mInKWqX7rvJCKzgdkACQkJXi7JmLYlp+gwLy/fzRsrMzlYXs2o+M48PfM0LhrR\ng9DgBtalr6mGXf+D9W/Ato+gpgJih8DFf4GR10B4hxb9DKb18iT09wDuR3jiXdvqMxO4w+3xFcAK\nVS0BEJFFwETgmNBX1WeBZwGSk5PtIK8JSN9kHuD5Zeks2rQPVWX68O7cemYSpydENTxev2+TE/Qb\n3oLSXGjXFcZ8D0bNcs6etSEcU4cnob8aGCAiSThhPxO4tm4jERkMRAHL3TZnAj8Qkd/jDO+cDfz1\nVIs2xl/U1CqLNu3l+WXpfJNZRMeIEG49M4kbJ/ap/7qzACW5sPFtWPeGMwMnKBQGXuAE/YDzIaSR\noR8T8JoMfVWtFpE7gU9wpmy+oKqbReRxIEVVF7qazgTm15mOuQCYCmzEOaj7sap+4NVPYEwbVF1T\ny8L1OTzzeSpp+aUkRrfnV5cN48ox8XQIr+fXsqocdixygj71v6A1Tk/+wj85J1FFRrf8hzBtkrS2\nKfPJycmakpLi6zKMaRZVNbX8+5s9zFmcSkZBGUN6dOLuqf25YFh3guquh6MK2Smw/nXY9I5zhaqO\nPZwx+lGzIG6wbz6EaZVEZI2qJjfVzs7INaYFVFbX8s7abOYsTiX7wGFG9OrM3BuTOW9I3PHj9UVZ\nzjLG6+dDQSqEtHPOlh01C/qe4yyAZsxJstA3phlVVNfwVko2/1icSk5xOaN6d+HXM4ZzzqDYY8O+\nogS2LoR1r8PuZYBCn0kw6V5nhcuIJtbPMcZDFvrGNIPyqhrmr8rkn0vS2HewnDF9onjiypHHnzmb\nuxW++j/Y8j5UlTrz6M95BEZ+F7om+ax+478s9I3xosOVNcxbmcG/lqaRd6iC8Uld+ct3RzGxX/Tx\nYb/kD7D5PQhtDyOuhFHX2mJnptlZ6BvjBaUV1by2IoO5X6aRX1LJpP7R/G3WaCb0rTOrZv8WJ+y3\nvA9hkXDW/TDxTmjf1TeFm4BjoW/MKThUXsUryzN47ss0DpRVMXlgLHdP7U9yYp0QPxr270FYBwt7\n4zMW+sachOLDVbz89W6eX5ZO8eEqpg6O466p/RmdEHVsw/2b3Xr2HeGsB2HiHRb2xmcs9I05AUVl\nlbzw1W5e/CqdQ+XVTBvajbunDmBEfOdjG+7b5IT91oVO2E/+MUy43cLe+JyFvjEeyCos47WVGcxb\nkUlJRTUXDu/OnVP7M6ynhb1pWyz0jWlATa2yeFsu81Zm8MWOPAS4cEQP7p46gEHd66xjv2+jK+w/\ngPBOMPkhZzljC3vTyljoG1NH7qFy3lyVxRurMskpLieuYzh3TR3ArHG96dG53bGN925wwn7bh07Y\nn/0TJ+zbRdX/4sb4mIW+MYCqsnxXAfNWZvLJ5n1U1ypn9o/hF5cO5dwh3Y5fx97C3rRRFvomoBWV\nVbJgTTavr8wkLb+ULu1DuXlSIteO70NSTOTxOxwT9p3h7Idhwm0W9qbNsNA3AUdVWZdVxGsrMvlw\nQw4V1bWcntCFv3x3FBeN6EFEaD0Lmu3bCF888W3Yn/MIjL8N2nVp+Q9gzCmw0DcBo7SimvfX5TBv\nZQabcw4SGRbMVWPiuW58H4b2bGBBs/1bYMkTzjx7C3vjByz0jU9UVteSklHI1r2H6BQRQnSHMKLa\nhxEdGU5UZCgdwkMavkTgCdq+7xDzVmbw7to9lFRUM7h7R359+XCuGN2r/guWAORtd3r2m//tnEE7\n+SGYeLsN45g2z0LftAhVZXdBGUt35LF0Rx7L0wooq6xpsH1YcBBRkaHOF8HRL4QwoiK/ve165Ke9\n89j9YGtFdQ2LNu5j3soMVu8+QFhIEJeM6MF1ExIav+ZsfqozZr/xbWchNFsuwfgZC33TbA6VV/H1\nrgIn6HfmkVV4GICEru35zum9OGtALKcnRHG4soaC0goOlFVSUFLp3JZWcqC0kkLXT07RQQpKKjhY\nXt3g+3WMCDn6hZBRUEZhaSV9otvz04sGc9WY3nSNbOTasYVpsORPzsVLQiJg0t1wxt0QGePtfxZj\nfMpC33hNba2yKafY1ZvPZ23mAaprlfZhwZzRL5ofnNWXyQNiSaxnVkxCdAMXAa+jqqaWA2WVHCit\ncr4oSqsoLK2g8MhtWRUHSiuZ2C+amWN7M6lfzPGXIXR3IAOW/sm5eElwqHP27KR7oEPcyf4zGNOq\nWeibU5J7sJylO/NZuiOPZan5FJZWAjCsZyd+MNkJ+TF9oggLCWrilTwTGhxEXMcI4jpGAB2bbN+g\noiz48s/wzasgwTDuB3DmfdCxu1fqNKa1stA3J6SiuoaU3QdYuiOPJTvy2LbvEAAxHcI4e2AskwfG\ncGb/WGI7hvu40gYczIEv/wJrX3YuPD7mJjjzfujcy9eVGdMiPAp9EZkOPA0EA8+p6hN1nn8KmOJ6\n2B6IU9UuIjIFeMqt6WBgpqq+d8qVmxahquzYX8JXqfl8udM5AFteVUtosJDcpys/mT6YyQNjGNK9\nU+PDKL52aB8sewpSXgStgdE3wFkPQJfevq7MmBbVZOiLSDAwB5gGZAOrRWShqm450kZV73Nrfxcw\n2rV9MXCaa3tXIBX41JsfwHiXqpJRUMbXuwr4elc+K9IKyC9xhmySYiK5Jrk3kwfGMqFvNJENTXds\nSm0t5G2DzK8hf6czDTIyBtrHQGSs6ycGIrpA0CkOC5XkwVd/hdXPQU0VnHYtTH7QuRatMQHIk9/a\ncUCqqqYBiMh8YAawpYH2s4DH6tl+FbBIVctOplDTfHKKDvP1rgKW7ypg+a58corLAejWKZyzBsQy\nsV80E/tG07urZwdbj1NdCXvXQcbXkLkcMldAeZHzXGikc0Hw+gSFuL4Ijvy4fSFExrp9SbhuwyK/\nvb5saQF8/TSsmgvV5TByJpz9Y+ja9+Q+gzF+wpPQ7wVkuT3OBsbX11BE+gBJwOf1PD0T+MuJFmi8\nL7+kguW7ClxBn8/uAud7uGtkGBP7RnN7v2jO6BdNUkzkyZ0gVXEIslY64Z6xHPakOMELEN0fhlwK\nCROhz0SISoLaGigrgNI858f9fmkelOY7twcynPuVh+p/35B2ri+BaOcviMpSGHG1sxhaTP+T/Ncy\nxr94+0DuTGCBqh5z1o2I9ABGAJ/Ut5OIzAZmAyQkJHi5JFN8uIqVaQVHe/Pb9zuh2TE8hPF9u3LD\nxETO6BfNoG4dT25cviTX1Ytf4QzZ7NsIWgsSBN1HQvItTsgnTKh/KmRwCHTs5vx4ourwt18EpflQ\nln/8F8SQwc5snNhBJ/55jPFjnoT+HsD9aFe8a1t9ZgJ31LP9u8C/VbWqvp1U9VngWYDk5GT1oCbT\niNKKalbvLjzam9+cU0ytQkRoEGMTu3L56F6c0S+aYT07EVJ3yeCmqDonMmUud3rxmcuhcJfzXEg7\niE92rgPbZyLEj4XwU5hW2ZDQds4BWDsIa8wJ8yT0VwMDRCQJJ+xnAtfWbSQig4EoYHk9rzELeOQU\n6jT1qKqpJaOgjNTcEnbllbArt4TUvBK25BykulYJDRZGJ0Rx97kDOKNfDKN6dyY8pJ4VJBujCrlb\nYfeXkPGV05sv2e881y7K6cGP+R4knAE9RkFII2e9GmN8rsnQV9VqEbkTZ2gmGHhBVTeLyONAiqou\ndDWdCcxX1WN66iKSiPOXwhJvFh5ISiqq2eUK9lS324yCMqprv/3n7t4pgv5xHfjB5L6c0S+a5D5d\naRd2giEPcGA3pC2B9CWQvtS/P8p0AAAO8UlEQVQZLgHo3BuSznZ68QkTIWbQqc+uMca0KKmT0T6X\nnJysKSkpvi6jxakqeYcqjgn1XXmlpOaWsO9g+dF2IUFCn+j29I/rQL/YDkdv+8V1aHjFyKYc2u+E\ne7or6Isyne0dukHSZCfokyZDVB8vfFJjTHMQkTWqmtxUOzsj1weqampZm3GAtZlFx/TeD7ktJtYh\nPIR+sZGc0T/6mHDvE93++Ev3najDRc5QTZqrJ5+31dke0RkSz4KJdzkhHzvo2ymQxhi/YKHfQvIO\nVbBkRx6Lt+WydGfe0YCP6xhO/7gOXH5ar2N67906hXttPXkqy5wplOlLnKDfu86ZXRPSzhmqGXWN\n05vvMQqCTmI4yBjTZljoN5PaWmXDnmI+35bLF9tz2ZBdDDghf9HwHkwZHMvEvjF0bh/q/TevqYI9\na78dk89aCTWVzslO8WNh8o+dkI9PhpBWukaOMaZZWOh7UXFZFUt25vHFtlyW7MijoLSSIIHRCVE8\neP5AzhkUx7CenbzXgwfnbNf8HbB/k/OzbxNkr4bKEkCg+wgY/0Mn5BMmQngH7723MabNsdA/BarK\n1r2HWLzd6c2vyThArUJU+1DOHhjLlMFxTB4QS1RjF+/w/M2cqZJHgn3/ZucnfzvUuo4FBIdD3GAY\neQ30PdsZn7crPhlj3Fjon6CSimq+Ss3ni+25LN6Wd3RmzfBenbhjSn+mDI5jVHwXgk9lxcmqcmdB\nsiPBvn+jc1tW8G2bTr2g23AYeAF0G+bcj+7vnN1qjDENsITwQFpeCZ9vy2Xx9lxWpRdSVaN0CA/h\nrAExTBkcxzkDY4nrFHHiL6wKB/e4gt29977TWf4XnIOtcUNg0EXOUE23YRA31HrwxpiTYqHfhP9t\n3c+tLzvnDQzs1oFbJiVxzqA4khOjPJs6WVvjBPuBDOekpyLX7YHdTrgfWW0SoHMCdB/uLEh2pPfe\nta/NqDHGeI2FfhPmfplG767teP37ExpeWvjwAVeQuwX6kXAvyoJatyWHJAg6xzvruQ+73An2bsOh\n21BnnrwxxjQjC/1GpOYeYkVaIY+c34/euhdS0+sP9/LiY3ds19U5e7XHKBg6A7r0cUI+KtEJ/OBm\nmKZpjDEesNCvLINDe51rpx7McYZiDubAob20y0xlVfg+YpcWw1K35SqCw74N8t7jvg30Ln2csLce\nuzGmlfLf0FeFioNuQb73uFDn4B5naKauiM7UdujB7rJwgrqcQdxpI50wPxLuHbrbQmPGmDbJf0K/\nrBA++ZkT5Ed67pUlx7eLjIVOPZ0VI3uPd+536gWdeji3HXtAeAcWpGTx0IINvPW9iZBkM2WMMf7B\nf0I/OAzSvoDOvZwpjv3OdQW6W6h37OHxsgPzVmQwsFsHxiZGNW/dxhjTgvwn9MM7wANbvfJSG7OL\nWZ9dzK8uG+bdJROMMcbHbGC6HvNWZtAuNJgrTu/l61KMMcarLPTrOFhexfvrcphxWk86RdjUSmOM\nf7HQr+Pfa/dwuKqG68bbVaKMMf7HQt+NqjJvZQaj4jszIt7m2htj/I+FvpvVuw+wY3+J9fKNMX7L\nQt/NvJUZdIwI4dJRPX1dijHGNAuPQl9EpovIdhFJFZGH63n+KRFZ5/rZISJFbs8liMinIrJVRLaI\nSKL3yveegpIKFm3cx5Wnx9MuzFa1NMb4pybn6YtIMDAHmAZkA6tFZKGqbjnSRlXvc2t/FzDa7SVe\nAX6rqp+JSAeg1lvFe9Pba7KprKnl+gkJvi7FGGOajSc9/XFAqqqmqWolMB+Y0Uj7WcAbACIyFAhR\n1c8AVLVEVctOsWavq61VXl+ZyfikrvSP6+jrcowxptl4Evq9gCy3x9mubccRkT5AEvC5a9NAoEhE\n3hWRb0TkT66/HOruN1tEUkQkJS8v78Q+gRd8mZpPZmEZ10+wA7jGGP/m7QO5M4EFqkeu9UcIcBbw\nIDAW6AvcVHcnVX1WVZNVNTk2NtbLJTXttRUZxHQI44Jh3Vv8vY0xpiV5Evp7gN5uj+Nd2+ozE9fQ\njks2sM41NFQNvAecfjKFNpe9xYf539b9fDe5N2EhNpnJGOPfPEm51cAAEUkSkTCcYF9Yt5GIDAai\ngOV19u0iIke671OBLXX39aU3VmWhwKxxdgDXGOP/mgx9Vw/9TuATYCvwlqpuFpHHReQyt6Yzgfmq\nqm771uAM7fxPRDYCAsz15gc4FVU1tcxflck5A2Mbvv6tMcb4EY+WVlbVj4CP6mz7RZ3Hv2xg38+A\nkSdZX7P639b95B6q4Hd2Bq4xJkAE9CD2vJWZ9OwcwZTBcb4uxRhjWkTAhn56filf7sxn1rgEgoPs\nQinGmMAQsKH/xqpMQoKEa8b2brqxMcb4iYAM/fKqGt5OyeL8Yd2I6xTh63KMMabFBGToL9q0lwNl\nVVxvB3CNMQEmIEP/tRWZ9I2JZGK/aF+XYowxLSrgQn/r3oOsyTjAteMTELEDuMaYwBJwoT9vZQbh\nIUFcNSbe16UYY0yLC6jQL6mo5t9r93DJyJ50aR/m63KMMabFBVTov79uD6WVNVxnF0oxxgSogAl9\nVeW1FZkM7dGJ0b27+LocY4zxiYAJ/W+yiti69yDXTbADuMaYwBUwoT9vRSYdwkOYcVq9F/0yxpiA\nEBChX1RWyYcbcrh8dE86hHu0sKgxxvilgAj9BWuyqaiu5To7A9cYE+D8PvRVlddXZjKmTxRDenTy\ndTnGGONTfh/6y3cVkJZfyvU2TdMYY/w/9F9bmUFU+1AuHN7D16UYY4zP+XXo5x4s59PN+7k6uTcR\nocG+LscYY3zOr0P/zdVZVNcqs8bZ0I4xxoAfh35NrfLGqkzOGhBDUkykr8sxxphWwaPQF5HpIrJd\nRFJF5OF6nn9KRNa5fnaISJHbczVuzy30ZvGNWbwtl5zicq4bb718Y4w5oskzlUQkGJgDTAOygdUi\nslBVtxxpo6r3ubW/Cxjt9hKHVfU075XsmXkrM+jWKZxzh3Rr6bc2xphWy5Oe/jggVVXTVLUSmA/M\naKT9LOANbxR3srIKy/hiRx7XjE0gNNhvR7CMMeaEeZKIvYAst8fZrm3HEZE+QBLwudvmCBFJEZEV\nInJ5A/vNdrVJycvL87D0hr2xKhMBZo3rfcqvZYwx/sTb3eCZwAJVrXHb1kdVk4Frgb+KSL+6O6nq\ns6qarKrJsbGxp1RAZXUtb6Vkce6QbvTo3O6UXssYY/yNJ6G/B3DvMse7ttVnJnWGdlR1j+s2DfiC\nY8f7ve6TzfvIL6nk+gm2zo4xxtTlSeivBgaISJKIhOEE+3GzcERkMBAFLHfbFiUi4a77McAkYEvd\nfb3ptRUZJHRtz1n9Y5rzbYwxpk1qMvRVtRq4E/gE2Aq8paqbReRxEbnMrelMYL6qqtu2IUCKiKwH\nFgNPuM/68bbU3EOsTC/k2vEJBAXZhVKMMaYujxaXV9WPgI/qbPtFnce/rGe/r4ERp1DfCXltRSZh\nwUFcPSa+pd7SGGPaFL+Zz3i4soZ31mZz4YjuRHcI93U5xhjTKvlN6B8sr+LsgbF2ANcYYxrhN9cO\n7NYpgmeuPd3XZRhjTKvmNz19Y4wxTbPQN8aYAGKhb4wxAcRC3xhjAoiFvjHGBBALfWOMCSAW+sYY\nE0As9I0xJoDIseuj+Z6I5AEZp/ASMUC+l8ppbm2pVmhb9balWqFt1duWaoW2Ve+p1NpHVZu8IEmr\nC/1TJSIprou2tHptqVZoW/W2pVqhbdXblmqFtlVvS9RqwzvGGBNALPSNMSaA+GPoP+vrAk5AW6oV\n2la9balWaFv1tqVaoW3V2+y1+t2YvjHGmIb5Y0/fGGNMA/wm9EVkuohsF5FUEXnY1/U0RkR6i8hi\nEdkiIptF5B5f19QUEQkWkW9E5ENf19IUEekiIgtEZJuIbBWRib6uqSEicp/r/4FNIvKGiET4uiZ3\nIvKCiOSKyCa3bV1F5DMR2em6jfJljUc0UOufXP8fbBCRf4tIF1/W6K6+et2ee0BEVERivP2+fhH6\nIhIMzAEuBIYCs0RkqG+ralQ18ICqDgUmAHe08noB7gG2+roIDz0NfKyqg4FRtNK6RaQXcDeQrKrD\ngWBgpm+rOs5LwPQ62x4G/qeqA4D/uR63Bi9xfK2fAcNVdSSwA3ikpYtqxEscXy8i0hs4H8hsjjf1\ni9AHxgGpqpqmqpXAfGCGj2tqkKruVdW1rvuHcEKpl2+rapiIxAMXA8/5upamiEhnYDLwPICqVqpq\nkW+ralQI0E5EQoD2QI6P6zmGqi4FCutsngG87Lr/MnB5ixbVgPpqVdVPVbXa9XAFEN/ihTWggX9b\ngKeAh4BmOeDqL6HfC8hye5xNKw5RdyKSCIwGVvq2kkb9Fed/wlpfF+KBJCAPeNE1HPWciET6uqj6\nqOoe4EmcHt1eoFhVP/VtVR7ppqp7Xff3Ad18WcwJuAVY5OsiGiMiM4A9qrq+ud7DX0K/TRKRDsA7\nwL2qetDX9dRHRC4BclV1ja9r8VAIcDrwD1UdDZTSeoYfjuEaC5+B80XVE4gUket9W9WJUWf6X6uf\nAigiP8MZVp3n61oaIiLtgZ8Cv2jO9/GX0N8D9HZ7HO/a1mqJSChO4M9T1Xd9XU8jJgGXichunGGz\nqSLymm9LalQ2kK2qR/5yWoDzJdAanQekq2qeqlYB7wJn+LgmT+wXkR4ArttcH9fTKBG5CbgEuE5b\n9xz1fjgdgPWu37d4YK2IdPfmm/hL6K8GBohIkoiE4RwMW+jjmhokIoIz5rxVVf/i63oao6qPqGq8\nqibi/Lt+rqqttjeqqvuALBEZ5Np0LrDFhyU1JhOYICLtXf9PnEsrPehcx0Lge6773wPe92EtjRKR\n6ThDk5epapmv62mMqm5U1ThVTXT9vmUDp7v+n/Yavwh914GaO4FPcH5p3lLVzb6tqlGTgBtwes3r\nXD8X+booP3IXME9ENgCnAb/zcT31cv01sgBYC2zE+X1sVWePisgbwHJgkIhki8itwBPANBHZifPX\nyhO+rPGIBmp9BugIfOb6PfunT4t000C9zf++rfuvHWOMMd7kFz19Y4wxnrHQN8aYAGKhb4wxAcRC\n3xhjAoiFvjHGBBALfWOMCSAW+sYYE0As9I0xJoD8P7sr6taqF3nQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results of the training.\n",
    "plt.plot(history.history['sparse_categorical_accuracy'], label=\"Training loss\")\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'], label=\"Validation loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Encoder_Decoder_model_sparse.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
